{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как кластер spark создался, можно начать пользоваться более удобным интерфейсом и работать в Jupyter, который хостится уже на кластере.\n",
    "\n",
    "Он немножко сломан по умолчанию, но на официальном форуме рассказали, что это легко починить:\n",
    "\n",
    "Необходимо подключиться к головной машине через ssh, открыть файл `/usr/bin/anaconda/lib/python2.7/site-packages/nbformat/_version.py` и заменить 5 на 4.\n",
    "\n",
    "После этого остается перезагрузить Jupyter через ambari."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сессия спарка доступна в ноутбуке через переменную `spark`.\n",
    "Для того, чтобы спарк \"прогрелся\" и начал выполнять запросы, создадим контекст, который нам впоследствии потребуется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sp.textFile(\"wasb:///seclog/day_01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество строк в файле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предыдущая задача решенная на спарке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_and_size(line):\n",
    "    columns = line.split(',')\n",
    "    user, size = columns[0], columns[8]\n",
    "\n",
    "    return user, float(size)\n",
    "\n",
    "result = data.map(get_user_and_size).reduceByKey(lambda x, y: x+y).values().mean()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что получилось гораздо приятнее и быстрее, чем класический MR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полный список операций, которые можно делать на спарке - здесь \n",
    "* https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "* http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем общее количество пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = data.map(lambda x: x.split(',')[0]).distinct().count()\n",
    "print(total_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем общее количество пользователей, которые сидят ночью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_night_users = data.filter(lambda x: int(x.split(',')[2].split(':')[0]) < 6).map(lambda x: x.split(',')[0]).distinct()\\\n",
    "                    .count()\n",
    "print(total_night_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем 10 самых больших документа, которые скачивали пользователи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_and_size(line):\n",
    "    columns = line.split(',')\n",
    "    name, size = columns[6], columns[8]\n",
    "    return float(size), name\n",
    "\n",
    "top_10_large_result = data.map(get_name_and_size).sortByKey(ascending=False).values()\\\n",
    "                      .zipWithIndex().filter(lambda x: x[1] < 10).keys().collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо того, что результат можно получить в Jupyter, его можно положить в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir -p /seclogres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data.map(get_name_and_size).sortByKey(ascending=False).values()\\\n",
    "        .zipWithIndex().filter(lambda x: x[1] < 10).keys()\n",
    "\n",
    "result.saveAsTextFile(\"wasb:///seclogres/top_10_requests.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /seclogres/top_10_requests.txt/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**\n",
    "\n",
    "* Посчитать топ 10 самых посещаемых страниц (считаются только успешные запросы - код 200)\n",
    "* Посчитать суммарное количество людей по часам (~гистограма)\n",
    "* Посчитать среднее количество людей по часам (~гистограма)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисления можно также проводить и в более \"ручном\" режиме (примерно как в MR)\n",
    "\n",
    "Ниже - вычисление среднего объема, который выкачивает каждый пользователь (смотри задачу выше), решенная немного другим подходом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    columns = line.split(',')\n",
    "    user, size = columns[0], columns[8]\n",
    "    return user, float(size)\n",
    "\n",
    "def sum_reducer(item):\n",
    "    key, values = item\n",
    "    result = 0\n",
    "    for value in values:\n",
    "        result += value\n",
    "    return result, 1\n",
    "\n",
    "def mean_reducer(item):\n",
    "    result_key, values = item\n",
    "    summ, count = 0, 0\n",
    "    for current_summ, current_count in values:\n",
    "        summ += current_summ\n",
    "        count += current_count\n",
    "    return summ / count\n",
    "\n",
    "\n",
    "result = data.map(mapper).groupByKey().map(sum_reducer).groupBy(lambda x: 1).map(mean_reducer).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**\n",
    "\n",
    "* Скачать все логи за год в кластер. Найти всех пользователей, которые заходили каждый день месяца с указанием - что за месяц (если такие есть). Если таких нет - найти пользователей, которые заходили наибольшее число раз в наибольшее количество дней (с указанием в какие месяца)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
