{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные, с которыми будет работать - открытые данные из комиссии по ценным бумагам и биржам США.\n",
    "Описанние данных лежит вот здесь - https://www.sec.gov/dera/data/edgar-log-file-data-set.html Здесь порядка терабайта логов с сервера за месяц, разбитых по дням.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно заранее создать папку для данных\n",
    "\n",
    "```\n",
    "hdfs dfs -mkdir -p /seclog\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: запускать необходимо с головной машины кластера\n",
    "```\n",
    "apt-get update && apt-get install parallel\n",
    "```\n",
    "\n",
    "```\n",
    "printf %s\\\\n {01..30} | parallel -k --lb 'wget http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/2017/Qtr2/log201706{}.zip && unzip -p log201706{}.zip log201706{}.csv | tail -n +2 | hdfs dfs -put - /seclog/day_{}.csv && rm log201706{}.zip'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для семинара** - чтобы не ждать слишком долго, можно сказать 3 первых дня для дальнейшей работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как кластер spark создался, можно начать пользоваться более удобным интерфейсом и работать в Jupyter, который хостится уже на кластере.\n",
    "\n",
    "Он немножко сломан по умолчанию, но на официальном форуме рассказали, что это легко починить:\n",
    "\n",
    "Необходимо подключиться к головной машине через ssh, открыть файл `/usr/bin/anaconda/lib/python2.7/site-packages/nbformat/_version.py` и заменить 5 на 4.\n",
    "\n",
    "После этого остается перезагрузить Jupyter через ambari."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сессия спарка доступна в ноутбуке через переменную `spark`.\n",
    "Для того, чтобы спарк \"прогрелся\" и начал выполнять запросы, создадим контекст, который нам впоследствии потребуется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sp.textFile(\"wasb:///seclog/day_01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество строк в файле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предыдущая задача решенная на спарке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_and_size(line):\n",
    "    columns = line.split(',')\n",
    "    user, size = columns[0], columns[8]\n",
    "\n",
    "    return user, float(size)\n",
    "\n",
    "result = data.map(get_user_and_size).reduceByKey(lambda x, y: x+y).values().mean()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что получилось гораздо приятнее и быстрее, чем класический MR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полный список операций, которые можно делать на спарке - здесь \n",
    "* https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "* http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем тогда запустить этот же алгоритм на всех данных, что у вас есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = sp.textFile(\"wasb:///seclog/day_*.csv\")\n",
    "result = full_data.map(get_user_and_size).reduceByKey(lambda x, y: x+y).values().mean()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы скачали хотя бы 3 дня из всего датасета, то можно заметить, что спустя некоторое время все грохнулось со страшной ошибкой\n",
    "\n",
    "```\n",
    "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 7.0 failed 4 times, most recent failure: Lost task 8.3 in stage 7.0 (TID 76, wn4-spark2.woms2y4mgyiehbhy33ebogdnwh.bx.internal.cloudapp.net, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
    "  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 377, in main\n",
    "    process()\n",
    "  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 372, in process\n",
    "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
    "  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 2499, in pipeline_func\n",
    "    return func(split, prev_func(split, iterator))\n",
    "  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 2499, in pipeline_func\n",
    "    return func(split, prev_func(split, iterator))\n",
    "  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 352, in func\n",
    "    return f(iterator)\n",
    "  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1861, in combineLocally\n",
    "    merger.mergeValues(iterator)\n",
    "  File \"/usr/hdp/current/spark2-client/python/pyspark/shuffle.py\", line 238, in mergeValues\n",
    "    for k, v in iterator:\n",
    "  File \"/usr/hdp/current/spark2-client/python/pyspark/util.py\", line 99, in wrapper\n",
    "    return f(*args, **kwargs)\n",
    "  File \"<stdin>\", line 5, in get_user_and_size\n",
    "ValueError: could not convert string to float: 'null'\n",
    "\n",
    "\tat org.apache.spark.api.python.BasePythonRunnerReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
    "    ...\n",
    "```\n",
    "\n",
    "Часто причину ошибки можно понять сразу из этого трейсбека, однако если это не получается сделать сразу - полезно пойти и посмотреть в консоль спарка.\n",
    "Конкретно здесь можно увидеть, что это питон жалуется на то, что строку null нельзя превратить в число.\n",
    "\n",
    "В такой ситуации мы можем предпринять самое простое - просто выкинуть такие элементы из датасета.\n",
    "\n",
    "Вариант 1 - закешировать результаты и использовать кешированный RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = full_data.filter(lambda x: x.split(',')[8] != 'null').cache()\n",
    "\n",
    "result = data.map(get_user_and_size).reduceByKey(lambda x, y: x+y).values().mean()\n",
    "print(result)\n",
    "\n",
    "data_count = data.count()\n",
    "print(data_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно отметить, что для подсчета общего количества не потребовалось еще раз фильтровать - спарк переиспользовал закешированный RDDшник. \n",
    "Однако кеш держится только в рамказ сессии - при перезапуске сессии, спарку придется пересчитать его заного. Более того, в процессе расчетов спарк может посчитать, что этот кеш нужно сбросить (например не будет места для его хранения) и тогда следующий расчет поверш кешированного rdd опять же будет его пересчитывать. \n",
    "\n",
    "Чтобы персистентно сохранить результат, можно явно записать результат в HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir -p /seclog/cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.saveAsTextFile(\"wasb:///seclog/cleaned/data.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /seclog/cleaned/data.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sp.textFile(\"wasb:///seclog/cleaned/data.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем общее количество пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = data.map(lambda x: x.split(',')[0]).distinct().count()\n",
    "print(total_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем общее количество пользователей, которые сидят ночью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_night_users = data.filter(lambda x: int(x.split(',')[2].split(':')[0]) < 6)\\\n",
    "                    .map(lambda x: x.split(',')[0]).distinct()\\\n",
    "                    .count()\n",
    "print(total_night_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем 10 самых больших документа, которые скачивали пользователи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_and_size(line):\n",
    "    columns = line.split(',')\n",
    "    name, size = columns[6], columns[8]\n",
    "    return float(size), name\n",
    "\n",
    "top_10_large_result = data.map(get_name_and_size).sortByKey(ascending=False).values()\\\n",
    "                      .zipWithIndex().filter(lambda x: x[1] < 10).keys().collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если результатом является не какая-то одна статистика, а большой массив данных, то его нужно сохранять в HDFS. В противном случае, у вас лопнет жупитер вместе с головной нодой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir -p /seclogres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data.map(get_name_and_size).sortByKey(ascending=False).values()\\\n",
    "        .zipWithIndex().filter(lambda x: x[1] < 10).keys()\n",
    "\n",
    "result.saveAsTextFile(\"wasb:///seclogres/top_10_requests.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /seclogres/top_10_requests.txt/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**\n",
    "\n",
    "* Посчитать топ 10 самых посещаемых страниц (считаются только успешные запросы - код 200)\n",
    "* Посчитать суммарное количество людей по часам (~гистограма)\n",
    "* Посчитать среднее количество людей по часам (~гистограма)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисления можно также проводить и в более \"ручном\" режиме (примерно как в MR)\n",
    "\n",
    "Ниже - вычисление среднего объема, который выкачивает каждый пользователь (смотри задачу выше), решенная немного другим подходом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    columns = line.split(',')\n",
    "    user, size = columns[0], columns[8]\n",
    "    return user, float(size)\n",
    "\n",
    "def sum_reducer(item):\n",
    "    key, values = item\n",
    "    result = 0\n",
    "    for value in values:\n",
    "        result += value\n",
    "    return result, 1\n",
    "\n",
    "def mean_reducer(item):\n",
    "    result_key, values = item\n",
    "    summ, count = 0, 0\n",
    "    for current_summ, current_count in values:\n",
    "        summ += current_summ\n",
    "        count += current_count\n",
    "    return summ / count\n",
    "\n",
    "\n",
    "result = data.map(mapper).groupByKey().map(sum_reducer).groupBy(lambda x: 1).map(mean_reducer).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно явно работать с партициями, которыми оперирует спарк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reducer_2(values):\n",
    "    summ, count = 0, 0\n",
    "    for s, c in values:\n",
    "        summ, count = summ + s, count + c\n",
    "    return summ / count\n",
    "\n",
    "result = data.map(mapper).groupByKey().map(sum_reducer).repartition(1).glom().map(mean_reducer_2).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Казалось бы, что мы уже попали с распределенный рай и можем ворочать большие данные, как нам вздумается. \n",
    "Однако ситуацию можно еще раз качественно улучшить!\n",
    "\n",
    "Спарк предоставляет возможность фиксировать формат датасетов в виде таблиц и умеет запускать SQL поверх этих таблиц. Таким образом, не требуется даже составлять спарковские операции в нужном порядке с ручным написанием функций - достаточно просто запустить один SQL запрос. Можно еще отдельно отметить, что работать это будет даже быстрее, так как у спарка есть возможноть оптимизировать запросы под капотом и не возиться с медленным питоном."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('wasb:///seclog/cleaned/data.bin', header=False, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['ip', 'date', 'time', 'zone', 'cik', 'accession', 'doc', 'code', 'size', 'norefer', 'noagent', 'find', 'crawler', 'browser']\n",
    "for index, name in enumerate(columns_name):\n",
    "    df = df.withColumnRenamed('_c{}'.format(index), name)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От датафрейма, можно вернутся к RDD, который лежит под этим датафреймом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы начать делать к нему запросы необходимо зарегистрировать его как временную таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = spark.sql(\"\"\"\n",
    "SELECT count(*) FROM logs\n",
    "\"\"\").toPandas()\n",
    "newdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решим задачу про среднее количество данных еще раз, но уже на SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "SELECT avg(size) \n",
    "FROM (\n",
    "    SELECT sum(size) as size\n",
    "    FROM logs\n",
    "    GROUP BY ip\n",
    ") as t\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**\n",
    "\n",
    "* Найти 10 пользователей (ip-адрессов), которые выкачали больше всего данных за все время используя Spark SQL.\n",
    "* Найти 10 пользователей (ip-адрессов), у которых самое большое время сессии на сайте (решать можно используя любые инструменты Spark). Сессия на сайте - серия запросов к серверу, сделанных одним пользователем в заданный промежуток времени. Сессия считается завершенной, если в течение 30 минут от пользователя не поступило к серверу ни одного нового запроса. Время сессии - время от запроса, который открыл сессию до последнего запроса в этой сессии.\n",
    "* Скачать все логи за год в кластер. Найти всех пользователей, которые заходили каждый день месяца с указанием - что за месяц (если такие есть). Если таких нет - найти пользователей, которые заходили наибольшее число раз в наибольшее количество дней (с указанием в какие месяца)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
