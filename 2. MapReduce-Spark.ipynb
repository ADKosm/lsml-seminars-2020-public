{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем кластер в облаке \n",
    "\n",
    "После создания можно подключиться к головной ноде - `ssh sshuser@<hadoop-name>-ssh.azurehdinsight.net`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВАЖНО**: При следующем создании кластера нужно указать ту же самую запись хранения и тот же самый контейнер, чтобы все данные, с которомы вы работали в HDFS сохранились и вы могли продожить с ними работу.\n",
    "\n",
    "Посмотреть на ваши данные в HDFS можно через BLOB-storage view в самом Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузим данные с твитами в хадуп"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sudo curl -O https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_{`seq -s , 1 13`}.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подформатируем\n",
    "```\n",
    "for i in {1..13}; do tail -n +2 IRAhandle_tweets_$i.csv > tweets_$i.data; done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отдельную папку для этих данных в HDFS\n",
    "```\n",
    "hdfs dfs -ls /\n",
    "hdfs dfs -mkdir -p /tweets/data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: все команды для hdfs смотреть здесь - https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заливаем данные\n",
    "```\n",
    "hdfs dfs -put tweets_* /tweets/data/\n",
    "hdfs dfs -ls /tweets/data/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем MR заадачи (например через гит или напрямую)\n",
    "```\n",
    "scp word-count.py sshuser@hadoop1-ssh.azurehdinsight.net:~/word-count.py\n",
    "scp top-20.py  sshuser@hadoop1-ssh.azurehdinsight.net:~/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем команду на запуск\n",
    "\n",
    "```\n",
    "yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files ~/word-count.py \\\n",
    "-mapper \"python3 word-count.py map\" \\\n",
    "-reducer \"python3 word-count.py reduce\" \\\n",
    "-input /tweets/data/ \\\n",
    "-output /tweets/result/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим результат\n",
    "```\n",
    "hdfs dfs -ls /tweets/result\n",
    "hdfs dfs -cat /tweets/result/part-00000 | head\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим второй шаг\n",
    "```\n",
    "yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"top-20\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator=\"+\" \\\n",
    "-files ~/top-20.py \\\n",
    "-mapper \"python3 top-20.py map\" \\\n",
    "-reducer \"python3 top-20.py reduce\" \\\n",
    "-input /tweets/result/ \\\n",
    "-output /tweets/top/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "hdfs dfs -cat /tweets/top/part-00000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы посмотреть на консоль хадупа можно\n",
    "* Добавить *.internal.cloudapp.net в прокси\n",
    "* Добавить headnodehost в прокси\n",
    "* Поднять прокси через головную ногу кластера\n",
    "* Открыть `http://headnodehost:19888/jobhistory`\n",
    "* Или открыть `http://hn1-hadoop.n3hsvtzmijuexf0fi4yqszkanb.bx.internal.cloudapp.net:8088/cluster` (ссылка может отличаться - она всегда пишется в консоли при запуске MR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**. \n",
    "\n",
    "* Запустить MapReduce задачу с предыдущего семинара (биграммы)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT (здесь хочется увидеть команды на запуск)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будет работать с данным из комиссии по ценным бумагам и биржам США\n",
    "\n",
    "Описанние данных лежит вот здесь - https://www.sec.gov/dera/data/edgar-log-file-data-set.html\n",
    "Здесь порядка терабайла логов с сервера за месяц, разбитый под дням."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно заранее создать папку для данных\n",
    "\n",
    "```\n",
    "hdfs dfs -mkdir -p /seclog\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: запускать необходимо с головной машины кластера\n",
    "```\n",
    "apt-get update && apt-get install parallel\n",
    "```\n",
    "\n",
    "```\n",
    "printf %s\\\\n {01..30} | parallel -k -lb 'wget http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/2017/Qtr2/log20170630.zip && unzip -p log201706{}.zip log201706{}.csv | tail -n +2 | hdfs dfs -put - /seclog/day_{}.csv && rm log201706{}.zip'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104.197.198.jbj,2017-06-01,00:00:00,0.0,1622116.0,0001609253-17-000117,-index.htm,301.0,682.0,1.0,0.0,0.0,10.0,0.0,\r\n",
      "104.197.198.jbj,2017-06-01,00:00:00,0.0,1622116.0,0001609253-17-000117,-index.htm,200.0,2878.0,1.0,0.0,0.0,10.0,0.0,\r\n",
      "104.247.35.caa,2017-06-01,00:00:00,0.0,1071522.0,0001047469-98-043476,.txt,200.0,8444.0,0.0,0.0,0.0,10.0,0.0,\r\n",
      "104.247.35.caa,2017-06-01,00:00:00,0.0,1071522.0,0001047469-98-043031,.txt,0.0,49756.0,0.0,0.0,0.0,10.0,0.0,\r\n",
      "107.170.205.bch,2017-06-01,00:00:00,0.0,102037.0,0001140361-17-023189,.txt,301.0,673.0,0.0,0.0,0.0,10.0,0.0,\r\n",
      "107.22.225.dea,2017-06-01,00:00:00,0.0,1576728.0,0001209191-17-036517,-index.htm,200.0,7702.0,1.0,0.0,0.0,10.0,0.0,\r\n",
      "107.23.85.jfd,2017-06-01,00:00:00,0.0,1353678.0,0000914121-06-000548,-index.htm,200.0,2688.0,1.0,0.0,0.0,10.0,0.0,\r\n",
      "107.23.85.jfd,2017-06-01,00:00:00,0.0,1059377.0,0001193125-15-111386,-index.htm,200.0,2741.0,1.0,0.0,0.0,10.0,0.0,\r\n",
      "107.23.85.jfd,2017-06-01,00:00:00,0.0,1059377.0,0001193125-16-523709,-index.htm,200.0,2745.0,1.0,0.0,0.0,10.0,0.0,\r\n",
      "107.23.85.jfd,2017-06-01,00:00:00,0.0,1059377.0,0001193125-14-121404,-index.htm,200.0,2743.0,1.0,0.0,0.0,10.0,0.0,\r\n",
      "107.23.85.jfd,2017-06-01,00:00:00,0.0,1059377.0,0001193125-17-101592,-index.htm,200.0,2741.0,1.0,0.0,0.0,10.0,0.0,\r\n",
      "107.23.85.jfd,2017-06-01,00:00:00,0.0,1353678.0,0000914121-06-000604,-index.htm,200.0,2694.0,1.0,0.0,0.0,10.0,0.0,\r\n",
      "107.23.85.jfd,2017-06-01,00:00:00,0.0,1353678.0,0000914121-06-000583,-index.htm,200.0,2686.0,1.0,0.0,0.0,10.0,0.0,\r\n",
      "107.23.85.jfd,2017-06-01,00:00:00,0.0,1059377.0,0001193125-13-135910,-index.htm,200.0,2742.0,1.0,0.0,0.0,10.0,0.0,\r\n",
      "107.23.85.jfd,2017-06-01,00:00:00,0.0,1353678.0,0000914121-06-000582,-index.htm,200.0,2693.0,1.0,0.0,0.0,10.0,0.0,\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /seclog/day_01.csv | head -n 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные аккуратно прилетели. Описание столбцов можно найти вот здесь - https://www.sec.gov/files/EDGAR_variables_FINAL.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важно (на семинаре)**: перед тем как начать решать следующую задачу, нужно поставить создаваться spark cluster - это долго."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**. \n",
    "\n",
    "* Посчитать на MapReduce средний объем данных (в байтах - смотри колонку size), который выкачивает каждый пользователь.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как кластер spark создался, можно начать пользоваться более удобный интерфейсом и работать в Jupyter, который хостится уже на кластере.\n",
    "\n",
    "Он немножко сломан по умолчанию, но на официальном форуме рассказали, что это легко починить\n",
    "\n",
    "Необходимо подключиться к головной машине через ssh, открыть файл `/usr/bin/anaconda/lib/python2.7/site-packages/nbformat/_version.py` и заменить 5 на 4.\n",
    "\n",
    "После этого остается перезагрузить jupyter через ambari."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сессия спарка доступна в ноутбуке через переменную `spark`.\n",
    "Для того, чтобы спарк \"прогрелся\" и начал выполнять запросы, создадим контекст, который нам впоследствии потребуется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sp.textFile(\"wasb:///seclog/day_01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество строк в файле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предыдущая задача решенная на спарке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_and_size(line):\n",
    "    columns = line.split(',')\n",
    "    user, size = columns[0], columns[8]\n",
    "\n",
    "    return user, float(size)\n",
    "\n",
    "result = data.map(get_user_and_size).reduceByKey(lambda x, y: x+y).values().mean()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что получилось гораздо приятнее и быстрее, чем класический MR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полный список операций, которые можно делать на спарке - здесь \n",
    "* https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "* http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем общее количество пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = data.map(lambda x: x.split(',')[0]).distinct().count()\n",
    "print(total_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем общее количество пользователей, которые сидят ночью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_night_users = data.filter(lambda x: int(x.split(',')[2].split(':')[0]) < 6).map(lambda x: x.split(',')[0]).distinct()\\\n",
    "                    .count()\n",
    "print(total_night_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем 10 самых больших документа, которые скачивали пользователи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_and_size(line):\n",
    "    columns = line.split(',')\n",
    "    name, size = columns[6], columns[8]\n",
    "    return float(size), name\n",
    "\n",
    "top_10_large_result = data.map(get_name_and_size).sortByKey(ascending=False).values()\\\n",
    "                      .zipWithIndex().filter(lambda x: x[1] < 10).keys().collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо того, что результат можно получить в Jupyter, его можно положить в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir -p /seclogres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data.map(get_name_and_size).sortByKey(ascending=False).values()\\\n",
    "        .zipWithIndex().filter(lambda x: x[1] < 10).keys()\n",
    "\n",
    "result.saveAsTextFile(\"wasb:///seclogres/top_10_requests.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n",
      ".txt\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /seclogres/top_10_requests.txt/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**\n",
    "\n",
    "* Посчитать топ 10 самых посещаемых страниц (считаются только успешные запросы - код 200)\n",
    "* Посчитать количество людей по часам (~гистограма)\n",
    "* Посчитать среднее количество людей по часам (~гистограма)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисления можно также проводить и в более \"ручном\" режиме (примерно как в MR)\n",
    "\n",
    "Ниже - вычисление среднего объема, который выкачивает каждый пользователь (смотри задачу выше), решенная немного другим подходом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    columns = line.split(',')\n",
    "    user, size = columns[0], columns[8]\n",
    "    return user, float(size)\n",
    "\n",
    "def sum_reducer(item):\n",
    "    key, values = item\n",
    "    result = 0\n",
    "    for value in values:\n",
    "        result += value\n",
    "    return result, 1\n",
    "\n",
    "def mean_reducer(item):\n",
    "    result_key, values = item\n",
    "    summ, count = 0, 0\n",
    "    for current_summ, current_count in values:\n",
    "        summ += current_summ\n",
    "        count += current_count\n",
    "    return summ / count\n",
    "\n",
    "\n",
    "result = data.map(mapper).groupByKey().map(sum_reducer).groupBy(lambda x: 1).map(mean_reducer).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**\n",
    "\n",
    "* Скачать все логи за год в кластер. Найти всех пользователей, которые заходили каждый день месяца с указанием - что за месяц (если такие есть). Если таких нет - найти пользователей, которые заходили наибольшее в наибольшее количество дней (с указанием в какие месяца)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
