{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом семинаре посмотрим на то, что можно сделать с большими данными, когда наши вычислительные возможности несколько ограничены. \n",
    "\n",
    "Ограниченость может быть вызвана различными факторами\n",
    "* У нас просто нет вычислительных ресурсов для hadoop кластера - есть только одна тачка для вычислений с жесткий диском\n",
    "* У нас все таки есть хадуп кластер, но при этом некоторые задачи все равно решаются на нем мучительно долго\n",
    "* У нас есть хадуп кластер, однако данные в огромных количествах прилетают каждую секунду (например сообщения из кафки или логи веб-серверов)\n",
    "* У нас есть только \"умная\" кофеварка и тонны данных для анализа - например мы строим Internet-of-things и хотим встроить какую-то аналитику в систему. В таком раскладе у нас есть небольшое устройство, которое подключено в гигантской сети из таких же устройств, каждое из которых непрерывно шлет показания с датчиков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далеко не любую задачу можно решить за приемлимое время с приемлимым качеством в таких условиях, однако существует целый класс алгоритмов, которые умеют выдавать разумные результаты в подобых сценариях - стриминговые алгоритмы.\n",
    "\n",
    "Основные отличия стриминговых алгоритмов следующие:\n",
    "* Память у алгоритма ограничена и много меньше размера входных данных\n",
    "* Алгоритм может посмотреть на данные только 1 раз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет на сегодняшний семинар - данные с сенсоров \"умного\" города в Денмарке. Сенсоры снимают показания о разруженности дорог по городу и сливают их в единый поток данных.\n",
    "\n",
    "Информация по датасету - http://iot.ee.surrey.ac.uk:8080/datasets.html#traffic\n",
    "\n",
    "Части датасета:\n",
    "```\n",
    "http://iot.ee.surrey.ac.uk:8080/datasets/traffic/traffic_feb_june/citypulse_traffic_raw_data_surrey_feb_jun_2014.tar.gz\n",
    "http://iot.ee.surrey.ac.uk:8080/datasets/traffic/traffic_june_sep/citypulse_traffic_raw_data_aarhus_aug_sep_2014.tar.gz\n",
    "http://iot.ee.surrey.ac.uk:8080/datasets/traffic/traffic_oct_nov/citypulse_traffic_raw_data_aarhus_oct_nov_2014.zip\n",
    "```\n",
    "\n",
    "Удобнее всего скачивать через wget:\n",
    "\n",
    "```bash\n",
    "wget -i links.txt\n",
    "\n",
    "tar -zxvf citypulse_traffic_raw_data_aarhus_aug_sep_2014.tar.gz\n",
    "tar -zxvf citypulse_traffic_raw_data_surrey_feb_jun_2014.tar.gz\n",
    "\n",
    "mkdir traffic_oct_nov\n",
    "unzip citypulse_traffic_raw_data_aarhus_oct_nov_2014.zip -d traffic_oct_nov\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уберем хедеры, чтобы они нам более не мешали при обработке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -i '1d' traffic_*/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Колонки датасета следующие\n",
    "```\n",
    "status\tavgMeasuredTime\tavgSpeed\textID\tmedianMeasuredTime\tTIMESTAMP\tvehicleCount\t_id\tREPORT_ID\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Простые статистики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Среднее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из самых простых стриминговых алгоритмов, который писал скорее всего каждый - это подсчет среднего значения набора чисел. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 0\n",
    "count = 0\n",
    "\n",
    "for number in number_stream:\n",
    "    value += number\n",
    "    count += 1\n",
    "    \n",
    "print(value / count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что алгоритм использует O(1) памяти (бкувально две переменные) и проходится по всем данных ровно один раз. \n",
    "Это практически эталонный пример того, как структурно выглядит стриминговый алгоритм и далее мы будем говорить именно о подобных алгоритмах.\n",
    "\n",
    "Реализуем подсчет среднего количества машин на нашем датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\r\n",
      "import csv\r\n",
      "\r\n",
      "vehicle_count = 0\r\n",
      "record_count = 0\r\n",
      "\r\n",
      "for record in csv.reader(iter(sys.stdin.readline, '')):\r\n",
      "    current_vehicle_count = int(record[6])\r\n",
      "    vehicle_count += current_vehicle_count\r\n",
      "    record_count += 1\r\n",
      "\r\n",
      "print(vehicle_count / record_count)\r\n"
     ]
    }
   ],
   "source": [
    "! cat py/mean-stream.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/tqdm:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.cli.*` instead of `tqdm._main.*`\n",
      "  from tqdm._main import main\n",
      "25097092it [00:38, 647576.76it/s]\n",
      "3.155170168719149\n"
     ]
    }
   ],
   "source": [
    "! cat traffic_*/* | tqdm | python3 py/mean-stream.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что мы решили эту задачу точно (не приближенно). Аналогично мы можем посчитать и другие несложные статистики - количество, минимум, максимум, дисперсию и так далее.\n",
    "\n",
    "Однако не все статистики считаются с такой легкостью. Например есть большие проблемы с подсчетом медианы в один проход. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сложные статистики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Медиана"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже предлагается к ознакомлению алгоритм для поиска медианы (Мунро-Патерсон).\n",
    "\n",
    "Идея крайне простая - возьмем T первых элементов из потока. Далее для всех следующих элементов будем подсчитывать, сколько из этих элементов больше (по значению), чем элементы из нашего множества и сколько элементов меньше. Если в конце окажется так, что и тех и тех (больших и меньших) элементов меньше, чем половина всех элементов в потоке (< N/2), то это означает, что наше множество содержит элементы как раз из середины упорядоченного ряда. А раз так, значит медиана - один из элементов нашего множества. Достаточно будет отсортировать наше множество и взять соответствующий элемент.\n",
    "\n",
    "Важно отметить, что будут элементы, которые не будут больше или меньше всех элементов нашего множества - они будут где-то между. В этот момент мы просто включим этот элемент в наше множество. Однако так как память у нас ограничена, то мы должны выкинуть какой-то элемент из множества, чтобы расход памяти не увеличивался. Легко заметить, что мы можем избавиться от минимального или максимального элемента нашего множества - если выкидываем максимум, то просто говорим, что на 1 увеличилось число элементов, больших чем наше (симметрично с минимумом). \n",
    "\n",
    "Осталось решить что выкинуть - минимум или максимум. Так как в конце мы бы хотели, чтобы больших и меньших элементов было примерно поровну, то тогда будем выкидывать элемент в соответствии с этим желанием - если меньших меньше, то выкидываем минимум, если больших - максимум.\n",
    "\n",
    "Более формальное описание алгоритма смотри здесь - https://www.cs.dartmouth.edu/~ac/Teach/data-streams-lecnotes.pdf\n",
    "\n",
    "Ниже - упрощенный схематичный пример работы алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Munro-Paterson](img/munro-paterson.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\r\n",
      "import csv\r\n",
      "\r\n",
      "MEMORY = 5000\r\n",
      "A = []\r\n",
      "\r\n",
      "\r\n",
      "stream = map(lambda x: int(x[6]), csv.reader(iter(sys.stdin.readline, '')))\r\n",
      "\r\n",
      "for _ in range(MEMORY):\r\n",
      "    A.append(next(stream))\r\n",
      "\r\n",
      "A_min = min(A)\r\n",
      "A_max = max(A)\r\n",
      "\r\n",
      "larger = 0\r\n",
      "less = 0\r\n",
      "N = len(A)\r\n",
      "\r\n",
      "for element in stream:\r\n",
      "    N += 1\r\n",
      "    if element > A_max:\r\n",
      "        larger += 1\r\n",
      "    elif element < A_min:\r\n",
      "        less += 1\r\n",
      "    else:\r\n",
      "        if less < larger:\r\n",
      "            A.remove(A_min)\r\n",
      "            A.append(element)\r\n",
      "            A_min = min(A)\r\n",
      "            less += 1\r\n",
      "        else:\r\n",
      "            A.remove(A_max)\r\n",
      "            A.append(element)\r\n",
      "            A_max = max(A)\r\n",
      "            larger += 1\r\n",
      "\r\n",
      "if less < N / 2 and larger < N / 2:\r\n",
      "    median_index = N // 2 - less\r\n",
      "    A.sort()\r\n",
      "    result = A[median_index]\r\n",
      "    print(result)\r\n",
      "else:\r\n",
      "    print(\"FAIL\")\r\n",
      "    print(N, less, larger, A_min, A_max)\r\n"
     ]
    }
   ],
   "source": [
    "! cat py/median-stream.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/tqdm:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.cli.*` instead of `tqdm._main.*`\n",
      "  from tqdm._main import main\n",
      "9305it [00:00, 31144.21it/s]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "! cat traffic_oct_nov/trafficData158324.csv | tqdm | python3 py/median-stream.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/tqdm:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.cli.*` instead of `tqdm._main.*`\n",
      "  from tqdm._main import main\n",
      "4382599it [00:28, 154355.97it/s]\n",
      "FAIL\n",
      "4382599 3051379 1326220 3 3\n"
     ]
    }
   ],
   "source": [
    "! cat traffic_oct_nov/* | tqdm | python3 py/median-stream.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/tqdm:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.cli.*` instead of `tqdm._main.*`\n",
      "  from tqdm._main import main\n",
      "4382599it [00:54, 81053.61it/s]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "! cat traffic_oct_nov/* | shuf | tqdm | python3 py/median-stream.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что алгоритм работает только на потоках, которые хорошо перемешаны. Если же нам с данными не повезет, то все пропало."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скетчи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Принадлежность множеству"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо подсчета каких-то статистик, часто возникает задача построить структуру данных, которая бы смогла отвечать нам на какие-то запросы после обработки потока.\n",
    "\n",
    "Например - **присутствовал ли такой элемент в потоке.**\n",
    "\n",
    "Если бы у нас было O(N) памяти, то тогда эту задачу можно было бы решить честно. Однако нам такой расклад не подходит, поэтому будем строить алгоритм, который отвечает на вопрос правильно с некоторой информацией, но при этом используя гораздо меньше памяти.\n",
    "\n",
    "Здесь нам могут помочь хеш-функции. Самая простая идея, которая нам может прийти в голову - запоминать не сами увиденные значения, а их хеши.\n",
    "\n",
    "План действий следующий: заведем массив размера T - здесь будем отмечать элементы, которые мы видели.\n",
    "Будем хешировать все элементы, которые есть в потоке, в отрезок [0, T] (T выбирается исходя из размера доступной памяти) и отмечать соответствующий элемент в нашем массиве как увиденный. После того, как мы обработаем таким образом весь массив мы можем обабатывать входящие запросы.\n",
    "\n",
    "Для входящего запроса посчитаем хеш элемента, который нас спросили и проверим, есть ли он у нас в массиве.\n",
    "Если в массиве указано, что такой хеш мы не видели, значит и сам элемент мы точно не видели - ответ нет.\n",
    "Если же указано, что видели - тогда возможно, что такой элемент присутствовал в потоке, а может и нет. Такое может произойти, когда хеш другого элемента из потока совпал с хешом элемента, про который спросили. В данной ситуации мы отвечаем, что видели, однако нужно держать в голове, что этот ответ может быть неверным с некоторой вероятностью.\n",
    "\n",
    "Для того, чтобы уменьшить вероятность ошибки, мы можешь применить следующий трюк - возьмем сразу P различных случайных хеш-функций. Будем отмечать в нашем массиве сразу все значения хешей, как увиденные.\n",
    "\n",
    "При ответе также возьмем все P хешей от элемента. Если хотя бы один из значений хеша отсутствует в нашем массиве, то значит такой элемент мы не видели.\n",
    "Если же все хеши присутствуют в массиве, значит или мы видели этот элемент, или нам очень не повезло и у нас случилось сразу P коллизий (что менее вероятно, чем в первом случае в одной хеш функцией).\n",
    "\n",
    "Структура данных, которую мы только что описали, называется **Bloom Filter**.\n",
    "\n",
    "Более подробное описание можно посмотреть здесь - https://shodhganga.inflibnet.ac.in/bitstream/10603/11703/9/09_chapter%204.pdf\n",
    "В статье также можно найти псевдокод работы алгоритма.\n",
    "\n",
    "Ниже - схема принципа работы струткуры.\n",
    "\n",
    "![Bloom-filter](img/bloom-filter.png)\n",
    "\n",
    "<sub><sup>Картинка взята из https://en.wikipedia.org/wiki/Bloom_filter</sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Может возникнуть справедливый вопрос - где взять много случайных хеш-функций. Самый простой вариант - взять в качестве хеш-функции парамеризуемую функцию и случайно выставлять параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_hash(size):\n",
    "    p1, p2, p3 = random.randint(10, 10**8), random.randint(10, 10**8), random.randint(10, 10**8)\n",
    "    \n",
    "    def _hash(value):\n",
    "        return (p1 + value * p2 + value ** 2 * p3) % size\n",
    "    \n",
    "    return _hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = generate_hash(20)\n",
    "h2 = generate_hash(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(h1(10))\n",
    "print(h2(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(h1(10))\n",
    "print(h1(21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**\n",
    "* Реализовать структуру bloom filter . Интерфейс алгоритма следующий - на stdin подается поток из датасета. Из потока необходимо выцепить колонку `REPORT_ID` (для нее считаем bloom-filter). Путь до файла с запросами будет передан через аргументы командной строки. Необходимо обработать поток и после дать ответы на запросы из файла. Шаблон алгоритма прилагается ниже - необходимо дореализовать его (шаблон можно изменять как угодно - он приведен только для примера)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import random\r\n",
      "import csv\r\n",
      "import sys\r\n",
      "\r\n",
      "\r\n",
      "def generate_hash(size):\r\n",
      "    p1, p2, p3 = random.randint(10, 10**8), random.randint(10, 10**8), random.randint(10, 10**8)\r\n",
      "\r\n",
      "    def _hash(value):\r\n",
      "        return (p1 + value * p2 + value ** 2 * p3) % size\r\n",
      "\r\n",
      "    return _hash\r\n",
      "\r\n",
      "MEMORY = 5000\r\n",
      "\r\n",
      "stream = map(lambda x: int(x[8]), csv.reader(iter(sys.stdin.readline, '')))\r\n",
      "\r\n",
      "for element in stream:\r\n",
      "    pass # DO IT\r\n",
      "\r\n",
      "query_file_path = sys.argv[1]\r\n",
      "with open(query_file_path, \"r\") as f:\r\n",
      "    for query in map(int, f):\r\n",
      "        print(\"NO\") # DO NOT FORGET TO ALSO UPDATE THIS\r\n"
     ]
    }
   ],
   "source": [
    "! cat py/bloom-filter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158324\r\n",
      "203546\r\n",
      "158776\r\n",
      "23\r\n",
      "894\r\n",
      "180926\r\n",
      "182984\r\n",
      "81511\r\n"
     ]
    }
   ],
   "source": [
    "! cat data/bloom-filter-query.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/tqdm:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.cli.*` instead of `tqdm._main.*`\n",
      "  from tqdm._main import main\n",
      "25097092it [00:33, 742842.06it/s]\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n"
     ]
    }
   ],
   "source": [
    "! cat traffic_*/* | tqdm | python3 py/bloom-filter.py data/bloom-filter-query.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсчет частоты "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададимся немного более сложным вопросом - **сколько раз заданный элемент встречался в потоке.**\n",
    "\n",
    "Как и с просто проверкой наличия элемента в потоке - эту задачу не получится решать честно в заданных условиях. Ответ опять будет примерный.\n",
    "\n",
    "Попробуем продолжить идею использования хешей для решения этой задачи. Опять возьмем массив размера T в который будем записывать, сколько раз мы увидели тот или иной хеш. Для каждого нового элемента считаем хеш и увеличиваем соответствующий счетчик в массиве.\n",
    "\n",
    "Когда нам придет запрос - посчитаем хеш и посмотрим в массив. Число будет скорее всего больше, чем правильный ответ, так как из-за коллизий в соответствующую ячейку добавились результаты от элементов, которые имеют одинаковый хеш.\n",
    "\n",
    "Для того, чтобы уменьшить масштаб трагедии из-за этих коллизий опять воспользуемся приемом с несколькими хеш-функциями. Возьмем теперь сразу несколько массивов и для каждого массива возьмем свою случайную хеш-функцию.\n",
    "Для каждого массива будем проделывать такие же операции.\n",
    "\n",
    "Теперь когда к нам придет запрос - посчитаем всех хеши от элемента и посмотрим во все соответствующие ячейки в массивах. Все эти значения очевидно не меньше чем правильный ответ, а значит минимум из этих чисел - наиболее точная оценка того, сколько на самом деле раз мы видели этот элемент в потоке. Его и дадим в качестве ответа.\n",
    "\n",
    "Та конструкция, которую мы только что построили, называется **Count Min Sketch**.\n",
    "\n",
    "Более подробное описание можно посмотреть здесь - http://resources.mpi-inf.mpg.de/departments/d1/teaching/ss13/gitcs/lecture5.pdf.\n",
    "\n",
    "Ниже - схема принципа работы структуры.\n",
    "\n",
    "![count-min-sketch](img/count-min-sketch.png)\n",
    "\n",
    "<sub><sup>Картика взята из https://github.com/gopalkrushnapattanaik/SystemDesign/wiki/Count-Min-Sketch</sup></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**\n",
    "* Реализовать структуру count min sketch. Интерфейс такой же как и у bloom filter. Посчитать нужно все также поверх колонки `REPORT_ID`.  Путь до файла с запросами будет передан через аргументы командной строки. Шаблон прикладывается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import random\r\n",
      "import csv\r\n",
      "import sys\r\n",
      "\r\n",
      "\r\n",
      "def generate_hash(size):\r\n",
      "    p1, p2, p3 = random.randint(10, 10**8), random.randint(10, 10**8), random.randint(10, 10**8)\r\n",
      "\r\n",
      "    def _hash(value):\r\n",
      "        return (p1 + value * p2 + value ** 2 * p3) % size\r\n",
      "\r\n",
      "    return _hash\r\n",
      "\r\n",
      "TABLE_SIZE = 5000\r\n",
      "HASHES_COUNT = 10\r\n",
      "\r\n",
      "stream = map(lambda x: int(x[8]), csv.reader(iter(sys.stdin.readline, '')))\r\n",
      "\r\n",
      "for element in stream:\r\n",
      "    pass # DO IT\r\n",
      "\r\n",
      "query_file_path = sys.argv[1]\r\n",
      "with open(query_file_path, \"r\") as f:\r\n",
      "    for query in map(int, f):\r\n",
      "        print(0) # DO NOT FORGET TO ALSO UPDATE THIS\r\n"
     ]
    }
   ],
   "source": [
    "! cat py/count-min-sketch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158324\r\n",
      "203546\r\n",
      "158776\r\n",
      "23\r\n",
      "894\r\n",
      "180926\r\n",
      "182984\r\n",
      "81511\r\n",
      "187774\r\n",
      "201855\r\n",
      "190100\r\n"
     ]
    }
   ],
   "source": [
    "! cat data/count-min-sketch-query.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/tqdm:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.cli.*` instead of `tqdm._main.*`\n",
      "  from tqdm._main import main\n",
      "25097092it [00:34, 717065.37it/s]\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "! cat traffic_*/* | tqdm | python3 py/count-min-sketch.py data/count-min-sketch-query.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количество уникальных элементов в потоке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одной из важный задач является подсчет **количества уникальных элементов в потоке.** В этот раз так просто составить табличку и просто хешировать в нее не получится. \n",
    "\n",
    "На лекции был рассказал алгоритм HyperLogLog, однако он не так прост для понимания и реализации. На семинаре предлагается рассмотреть более простой, но все еще работающий способ решения задачи.\n",
    "\n",
    "Давайте посмотрим на двоичную запись хеша от элемента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0b1100010100100\n",
      "0b110001011011\n",
      "0b1100011101010\n",
      "0b1011101000001\n",
      "0b11101100000\n",
      "0b1000001010111\n",
      "0b101100010110\n",
      "0b1111010101101\n",
      "0b10010000001100\n",
      "0b1101100110011\n"
     ]
    }
   ],
   "source": [
    "hash_function = generate_hash(10**4)\n",
    "\n",
    "for number in range(10):\n",
    "    print(bin(hash_function(number)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на количество нулей в конце двоичной записи.\n",
    "Если хеш случайный, то вероятность того, что на конце будет 1 = 1/2. \n",
    "Вероятность того, что на конце будет 10 = 1/4. 100 - 1/8 и так далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0b1100010100100 2\n",
      "0b110001011011 0\n",
      "0b1100011101010 1\n",
      "0b1011101000001 0\n",
      "0b11101100000 5\n",
      "0b1000001010111 0\n",
      "0b101100010110 1\n",
      "0b1111010101101 0\n",
      "0b10010000001100 2\n",
      "0b1101100110011 0\n"
     ]
    }
   ],
   "source": [
    "def zeros(number):\n",
    "    result = 0\n",
    "    while number and number & 1 == 0:\n",
    "        result += 1\n",
    "        number = number >> 1\n",
    "    return result\n",
    "\n",
    "for number in range(10):\n",
    "    print(bin(hash_function(number)), zeros(hash_function(number)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это так же означает, что в множестве элементов примерно половина будет с нулем 0 на конце их хеш-функции, примерно четверть с одним 0 на конце, восьмая часть с двумя 0 на конце и тд. Этот факт нас очень скоро потребуется.\n",
    "\n",
    "Итак, если бы у нас было неограниченно памяти, мы бы в таком случае просто складывали все элементы из потока в множество и в конце просто бы посмотрели на размер этого множества - это был бы честный ответ в данной задаче.\n",
    "\n",
    "Учитывая, что память у нас ограничена, нам придется каким-то образом ужимать это множество. План следующий:\n",
    "\n",
    "* Возмем пустое множество B и отдельный счетчик z = 0\n",
    "* Для входящих элементов из потока будет добавлять в множество этот элемент.\n",
    "* Как только мы увидим, что размер множества превзошел определенный порог (лимит по памяти) производим следующую операцию\n",
    "  * Из множества удаляем все элементы у которых ровно z нулей на конце хеша (в первый раз будет 0 нулей, то есть те, у которых хеш оканчивается на 1)\n",
    "  * Увеличиваем z на 1\n",
    "* Далее все следующие элементы добавляем в множество, только если количество нулей на конце хеша не меньше z. \n",
    "* Как только в следующий раз у нас опять множество \"переполняется\", вновь повторяем процедуру с очисткой множества и увеличения z\n",
    "* В конце необходимо по нашему получившемуся множеству и z восстановить, сколько же различных элементов мы видели\n",
    "\n",
    "Каждая операция очистки множества удаляла из него примерно половину элементов (это мы увидели выше). Таким образом, исходное количество различных элементов в множестве будет равно примерно |B| * 2^z.\n",
    "\n",
    "Это и будет нашим ответом.\n",
    "\n",
    "Описанный алгоритм называется BJKST.\n",
    "\n",
    "Более подробное описание можно посмотреть здесь - http://resources.mpi-inf.mpg.de/departments/d1/teaching/ss13/gitcs/lecture5.pdf. В статье также можно найти псевдокод работы алгоритма.\n",
    "\n",
    "Ниже - схема принципа работы струткуры.\n",
    "\n",
    "![bjkst](img/bjkst.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**\n",
    "* Реализовать алгоритм BJKST. Из потока необходимо выцепить колонку REPORT_ID (для нее считаем количество уникальных элементов). Шаблон прикладывается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import random\r\n",
      "import csv\r\n",
      "import sys\r\n",
      "\r\n",
      "\r\n",
      "def generate_hash(size):\r\n",
      "    p1, p2, p3 = random.randint(10, 10**8), random.randint(10, 10**8), random.randint(10, 10**8)\r\n",
      "\r\n",
      "    def _hash(value):\r\n",
      "        return (p1 + value * p2 + value ** 2 * p3) % size\r\n",
      "\r\n",
      "    return _hash\r\n",
      "\r\n",
      "def zeros(number):\r\n",
      "    result = 0\r\n",
      "    while number and number & 1 == 0:\r\n",
      "        result += 1\r\n",
      "        number = number >> 1\r\n",
      "    return result\r\n",
      "\r\n",
      "MEMORY = 5000\r\n",
      "\r\n",
      "stream = map(lambda x: int(x[8]), csv.reader(iter(sys.stdin.readline, '')))\r\n",
      "\r\n",
      "for element in stream:\r\n",
      "    pass # DO IT\r\n",
      "\r\n",
      "result = 0\r\n",
      "print(result)\r\n"
     ]
    }
   ],
   "source": [
    "! cat py/bjkst.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/tqdm:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.cli.*` instead of `tqdm._main.*`\n",
      "  from tqdm._main import main\n",
      "25097092it [00:34, 729667.56it/s]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "! cat traffic_*/* | tqdm | python3 py/bjkst.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А что делать, если в потоке содержатся не числа, а строки?\n",
    "\n",
    "**Задача**\n",
    "* Придумать, как модифицировать алгоритм, если в потоке не числа, а строки. Модифицированную версию алгоритма запустить на всем датасете, как на наборе строк (то есть не парсить csv таблицу, а считать что на каждой строчке просто какая-то текстовая строка). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
