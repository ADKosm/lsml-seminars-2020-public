{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня будем обрабатывать данные из Авито - https://www.kaggle.com/c/avito-context-ad-clicks .\n",
    "В датасете присутствуют связанные между собой таблицы, в которых лежит информация о том, как показывалась реклама пользователям и как они на нее кликали (или не кликали)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сами данные будет подготавливать в спарке, а обучать машину будем через VW. \n",
    "**Для семинара** - данные лучше загружать заранее. Для удобства, они выложены в открытый доступ с Azure.\n",
    "\n",
    "```\n",
    "https://hadoop2hdistorage2.blob.core.windows.net/spark2-2020-02-09t17-13-49-527z/avito/AdsInfo.tsv\n",
    "https://hadoop2hdistorage2.blob.core.windows.net/spark2-2020-02-09t17-13-49-527z/avito/Category.tsv\n",
    "https://hadoop2hdistorage2.blob.core.windows.net/spark2-2020-02-09t17-13-49-527z/avito/Location.tsv\n",
    "https://hadoop2hdistorage2.blob.core.windows.net/spark2-2020-02-09t17-13-49-527z/avito/PhoneRequestsStream.tsv\n",
    "https://hadoop2hdistorage2.blob.core.windows.net/spark2-2020-02-09t17-13-49-527z/avito/SearchInfo.tsv\n",
    "https://hadoop2hdistorage2.blob.core.windows.net/spark2-2020-02-09t17-13-49-527z/avito/trainSearchStream.tsv\n",
    "https://hadoop2hdistorage2.blob.core.windows.net/spark2-2020-02-09t17-13-49-527z/avito/UserInfo.tsv\n",
    "https://hadoop2hdistorage2.blob.core.windows.net/spark2-2020-02-09t17-13-49-527z/avito/VisitsStream.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сборка кролика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone --recursive https://github.com/VowpalWabbit/vowpal_wabbit.git\n",
    "! sudo apt install libboost-dev libboost-program-options-dev libboost-system-dev libboost-thread-dev libboost-math-dev libboost-test-dev zlib1g-dev cmake g++ -y\n",
    "! cd vowpal_wabbit && make && cd build && make install\n",
    "! echo \"export PATH=/usr/local/bin:\\$PATH\" >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или можно просто стащить уже собраный бинарь отсюда - http://finance.yendor.com/ML/VW/Binaries/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p bin/\n",
    "! wget http://finance.yendor.com/ML/VW/Binaries/vw-8.20190624 -O bin/vw\n",
    "! chmod +x bin/vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! echo \"export PATH=$(pwd)/bin:\\$PATH\" >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузим датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/0.18/datasets/rcv1.html\n",
    "\n",
    "Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-25 11:30:57--  http://hunch.net/~vw/rcv1.tar.gz\n",
      "Resolving hunch.net (hunch.net)... 188.138.121.136\n",
      "Connecting to hunch.net (hunch.net)|188.138.121.136|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456931808 (436M) [application/x-gzip]\n",
      "Saving to: ‘rcv1.tar.gz’\n",
      "\n",
      "rcv1.tar.gz         100%[===================>] 435.76M   755KB/s    in 14m 57s \n",
      "\n",
      "2019-12-25 11:45:54 (498 KB/s) - ‘rcv1.tar.gz’ saved [456931808/456931808]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget http://hunch.net/~vw/rcv1.tar.gz -O rcv1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rcv1/\n",
      "rcv1/rcv1.train.vw.gz\n",
      "rcv1/vw_process\n",
      "rcv1/rcv1.test.vw.gz\n"
     ]
    }
   ],
   "source": [
    "! tar -zxvf rcv1.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 |f 13:3.9656971e-02 24:3.4781646e-02 69:4.6296168e-02 85:6.1853945e-02 140:3.2349996e-02 156:1.0290844e-01 175:6.8493910e-02 188:2.8366476e-02 229:7.4871540e-02 230:9.1505975e-02 234:5.4200061e-02 236:4.4855952e-02 238:5.3422898e-02 387:1.4059304e-01 394:7.5131744e-02 433:1.1118756e-01 434:1.2540409e-01 438:6.5452829e-02 465:2.2644201e-01 468:8.5926279e-02 518:1.0214076e-01 534:9.4191484e-02 613:7.0990764e-02 646:8.7701865e-02 660:7.2289191e-02 709:9.0660661e-02 752:1.0580081e-01 757:6.7965068e-02 812:2.2685185e-01 932:6.8250686e-02 1028:4.8203137e-02 1122:1.2381379e-01 1160:1.3038123e-01 1189:7.1542501e-02 1530:9.2655659e-02 1664:6.5160148e-02 1865:8.5823394e-02 2524:1.6407280e-01 2525:1.1528353e-01 2526:9.7131468e-02 2536:5.7415009e-01 2543:1.4978983e-01 2848:1.0446861e-01 3370:9.2423186e-02 3960:1.5554591e-01 7052:1.2632671e-01 16893:1.9762035e-01 24036:3.2674628e-01 24303:2.2660980e-01\r\n",
      "0 |f 9:8.5609287e-02 14:2.9904654e-02 19:6.1031535e-02 20:2.1757640e-02 24:1.3484491e-02 39:5.0661467e-02 45:2.5001373e-02 49:6.0599580e-02 50:2.5216307e-02 55:4.5040369e-02 64:5.1786009e-02 65:1.2385602e-01 69:5.0108045e-02 70:3.8740762e-02 73:3.9056923e-02 82:8.0505788e-02 90:3.5707459e-02 99:4.7706675e-02 104:5.1884215e-02 105:1.5568805e-01 106:1.3511626e-01 130:1.0544337e-01 133:8.1089266e-02 171:5.9266284e-02 180:7.7435717e-02 217:6.8277337e-02 233:2.6908301e-02 234:3.5577789e-02 254:1.0109196e-01 286:3.6022667e-02 300:1.2054443e-01 305:3.7950054e-02 326:5.6624860e-02 337:6.8930335e-02 348:3.7790950e-02 400:4.4774704e-02 417:4.3467607e-02 434:8.2317248e-02 441:1.1299837e-01 465:9.6445926e-02 476:5.0166391e-02 481:1.0036784e-01 495:1.0148438e-01 497:4.0623948e-02 510:4.2873766e-02 514:4.4061519e-02 518:6.7046829e-02 548:9.7496063e-02 606:4.7255926e-02 655:5.1991425e-02 678:3.3746067e-02 724:3.5048731e-02 759:6.3103504e-02 768:4.6424236e-02 802:2.4630768e-02 820:4.3894887e-02 910:5.6646861e-02 934:7.6288253e-02 995:4.2333681e-02 1011:4.5684557e-02 1091:6.5844811e-02 1100:1.5925008e-01 1288:4.4281408e-02 1321:5.2120164e-02 1340:1.5260276e-01 1574:7.5433277e-02 1629:6.3325211e-02 1654:3.2249656e-02 1712:1.6111535e-01 1796:8.0251180e-02 1930:8.7665550e-02 2031:1.4472182e-01 2036:8.9130148e-02 2039:9.6134968e-02 2277:8.1756182e-02 2330:7.0579961e-02 2334:8.2388259e-02 2343:8.3097421e-02 2344:2.0492174e-01 2348:7.7909611e-02 2360:9.3834393e-02 2362:3.7530366e-02 2376:7.0720568e-02 2493:1.8794763e-01 2495:5.9343126e-02 2520:1.1095246e-01 2949:5.7359278e-02 3370:2.5833043e-01 4523:1.9765969e-01 4525:1.1142892e-01 5307:9.4257712e-02 5401:7.0384808e-02 5593:8.1356630e-02 6093:7.4096188e-02 9217:1.0568235e-01 11017:8.1550762e-02 12301:1.2676764e-01 12332:2.9680410e-01 12338:9.4060794e-02 12339:2.5156361e-01 12340:2.6923507e-01 12341:1.5476021e-01\r\n",
      "0 |f 13:4.6038497e-02 20:3.8479928e-02 24:4.0378645e-02 55:4.7046758e-02 90:1.0692423e-01 121:7.5637124e-02 147:6.1341532e-02 161:1.2443262e-01 166:9.2344858e-02 179:5.2878667e-02 188:3.2931156e-02 230:6.2741712e-02 236:5.2074093e-02 240:5.4761782e-02 298:1.2427004e-01 315:7.1311563e-02 350:1.3221318e-01 357:1.5712146e-01 368:1.0088409e-01 433:9.1585882e-02 545:7.7802099e-02 558:6.7504369e-02 573:6.1524265e-02 584:1.1028677e-01 631:9.3503900e-02 660:4.9565587e-02 678:5.9682313e-02 691:1.2557535e-01 820:1.6291782e-01 1059:5.0108973e-02 1139:1.4289856e-01 1145:7.3475920e-02 1183:1.0825604e-01 1188:1.0738946e-01 1272:1.2391161e-01 1335:1.0124054e-01 1529:2.1344736e-01 1910:1.0896006e-01 2066:1.1121049e-01 2090:9.1144264e-02 2162:1.7347734e-01 2812:1.1876234e-01 2841:1.0789524e-01 2871:1.1722078e-01 2878:2.2459558e-01 2903:1.2972572e-01 3006:3.1611764e-01 3008:2.2492266e-01 3394:1.1831106e-01 3762:1.6104744e-01 3845:1.1758391e-01 3852:3.2081831e-01 5118:3.1833124e-01 5651:1.3049011e-01 5656:2.2692566e-01\r\n",
      "0 |f 3:4.5577556e-02 24:1.1411044e-02 25:3.7974268e-02 33:2.8507719e-02 41:3.5226457e-02 42:4.4543922e-02 50:3.6129922e-02 52:3.1403832e-02 55:2.2511169e-02 69:1.5188689e-02 76:4.0629715e-02 84:7.4837752e-02 85:3.4358751e-02 89:6.5839954e-02 110:2.9084764e-02 161:7.4031636e-02 170:3.5370197e-02 196:4.8320368e-02 208:3.9144717e-02 209:5.5873640e-02 269:4.3558568e-02 276:7.6310657e-02 280:3.7778754e-02 286:3.0483631e-02 288:2.4288468e-02 293:2.3107700e-02 298:3.5118837e-02 314:1.8277630e-01 319:3.1361207e-02 334:6.3679650e-02 336:3.3797063e-02 354:3.5158016e-02 357:4.4402681e-02 389:4.5763917e-02 428:7.6703973e-02 430:6.6179007e-02 433:2.5882259e-02 464:2.3039971e-02 475:5.8085274e-02 476:4.2452544e-02 483:6.5434702e-02 494:3.7052151e-02 533:2.2281876e-02 575:3.5521347e-02 664:4.0118020e-02 678:2.8557094e-02 690:4.1161560e-02 757:3.7753370e-02 802:4.3742239e-02 803:4.1759558e-02 814:6.5572858e-02 836:4.2416338e-02 857:5.1457152e-02 871:4.5915388e-02 963:3.9394960e-02 973:9.8459624e-02 1029:3.6264464e-02 1059:4.0595561e-02 1094:3.9410282e-02 1110:4.2543653e-02 1122:2.8821396e-02 1141:5.7815406e-02 1173:3.5140518e-02 1188:3.0348364e-02 1213:4.0825505e-02 1237:4.2819086e-02 1266:1.1101459e-01 1308:4.8654899e-02 1342:8.2725197e-02 1417:7.9656065e-02 1419:8.4478162e-02 1422:3.3052005e-02 1434:4.7695637e-02 1459:5.6947891e-02 1461:6.3311882e-02 1465:9.5394187e-02 1488:5.8578175e-02 1623:3.8236793e-02 1723:5.8299869e-02 1730:5.7023022e-02 1881:5.3735066e-02 2028:4.5234472e-02 2223:1.0081239e-01 2234:4.1061066e-02 3013:1.0455123e-01 3015:1.2765741e-01 3137:4.5956153e-02 3188:7.3665448e-02 3291:5.2890804e-02 3298:5.7951961e-02 3380:1.0363416e-01 3384:2.0652667e-01 3386:1.1103135e-01 3390:6.5076917e-02 3410:1.4741886e-01 3474:7.3495716e-02 3483:1.8200213e-01 3533:9.9155381e-02 3562:2.3795952e-01 3615:6.1804771e-02 3899:6.2048983e-02 4003:6.2531702e-02 4038:9.2995450e-02 4194:6.7665890e-02 4206:5.8146089e-02 4345:6.6447310e-02 4724:7.4850418e-02 4852:7.5302698e-02 5651:6.2437568e-02 5671:1.2755182e-01 5845:1.3229835e-01 6047:6.8817139e-02 6319:6.4124115e-02 8277:9.3362913e-02 8318:1.6047120e-01 8408:1.1899689e-01 8472:8.1823200e-02 8651:1.1915996e-01 9041:1.1209033e-01 9095:7.3802844e-02 9251:9.2515543e-02 9252:9.7969875e-02 12632:8.0214806e-02 18535:8.5983023e-02 18618:1.1447286e-01 18778:2.0379353e-01 22746:9.7772084e-02 31213:1.1904188e-01 31214:2.0987929e-01 36203:1.3161305e-01 36354:2.9326519e-01 36831:1.7453752e-01 36832:2.1526906e-01\r\n",
      "0 |f 23:5.8031932e-02 42:4.3960683e-02 45:2.0880008e-02 50:3.5656851e-02 52:3.0992642e-02 69:1.4989815e-02 73:1.9265046e-02 101:3.6552731e-02 131:8.4322646e-02 161:2.7999246e-02 169:3.0079322e-02 179:5.2403107e-02 182:6.2946759e-02 188:1.5550749e-02 193:3.1950910e-02 225:3.7829999e-02 233:2.2472586e-02 236:5.1605769e-02 269:2.5389541e-02 276:2.6976349e-02 298:5.8682792e-02 301:9.8974206e-02 312:3.7751961e-02 342:7.9125486e-02 378:4.9176287e-02 464:2.2738295e-02 478:3.8368121e-02 510:3.5806216e-02 516:3.9175954e-02 533:2.1990126e-02 557:5.4702684e-02 564:4.6411715e-02 642:8.1580848e-02 655:4.3420870e-02 673:3.8477339e-02 702:3.0201910e-02 751:4.8421662e-02 760:5.0214007e-02 768:3.8771410e-02 789:4.6299059e-02 800:6.4234152e-02 802:4.3169495e-02 836:4.1860957e-02 973:5.7390418e-02 1011:3.8153660e-02 1036:6.7988560e-02 1038:4.4288885e-02 1059:4.0064018e-02 1062:4.4851292e-02 1081:9.2638679e-02 1110:4.1986603e-02 1118:5.2589338e-02 1130:4.8003461e-02 1133:4.9178164e-02 1196:4.0469117e-02 1214:5.3173710e-02 1221:5.0143369e-02 1263:8.3388008e-02 1298:1.0334191e-01 1305:4.2508319e-02 1335:4.7807802e-02 1350:5.0633088e-02 1380:3.8872030e-02 1389:4.8115946e-02 1416:4.7130316e-02 1417:4.6430152e-02 1454:3.2705292e-02 1455:4.2858150e-02 1479:5.2930076e-02 1690:5.0393224e-02 1705:6.8371922e-02 1865:7.9661109e-02 1890:4.8129275e-02 2024:4.9730893e-02 2028:7.5585797e-02 2115:6.9944449e-02 2126:4.7789685e-02 2166:5.4787014e-02 2248:4.6545438e-02 2259:8.2881287e-02 2323:5.9373714e-02 2362:3.1343650e-02 2380:6.3111462e-02 2433:5.1356785e-02 2477:8.3841749e-02 2538:5.1214628e-02 2563:6.2386304e-02 2614:4.9308911e-02 3008:4.4509582e-02 3027:1.0765981e-01 3236:1.0831050e-01 3536:7.3078923e-02 3577:9.9980108e-02 3800:5.7047315e-02 3825:1.1666410e-01 3845:5.5525463e-02 4045:1.0749368e-01 4594:9.7434357e-02 4599:8.8552117e-02 5158:6.0078427e-02 5313:8.2510263e-02 5597:1.4109479e-01 6051:1.6858204e-01 6578:7.9471059e-02 7351:6.5885052e-02 8081:7.5523540e-02 8256:2.2026148e-01 8269:8.3418600e-02 8272:8.8049397e-02 8273:1.6269000e-01 8284:2.7112472e-01 8287:2.3736267e-01 8395:9.2970602e-02 8421:8.1172608e-02 8467:8.3752871e-02 8615:7.7104934e-02 10752:1.6747326e-01 13810:1.9023852e-01 14387:8.6804882e-02 18535:8.4857196e-02 18656:1.0005520e-01 18661:3.9166561e-01 18665:2.0358166e-01 18666:7.8335546e-02 18668:1.0148206e-01 20228:1.0928469e-01 24559:1.0928469e-01 25081:8.0901086e-02 33705:1.2259024e-01\r\n",
      "1 |f 5:1.1789641e-01 39:6.0373064e-02 45:1.3163488e-01 60:1.4378849e-01 69:8.6419873e-02 73:4.6543971e-02 140:4.2846322e-02 217:6.5645427e-02 218:6.7507431e-02 232:2.0380184e-01 257:6.6671841e-02 286:4.2928066e-02 288:9.8053738e-02 388:1.3288118e-01 400:2.6614136e-01 522:2.2009864e-01 588:1.8588908e-01 613:1.9352522e-01 655:6.1957959e-02 724:7.0718400e-02 842:5.3571172e-02 911:1.6738084e-01 1012:2.6197922e-01 1115:3.3340582e-01 1239:1.1640005e-01 2937:2.5619447e-01 4003:3.1289726e-01 10731:3.5240188e-01 12773:3.7005949e-01\r\n",
      "0 |f 15:3.5283413e-02 24:1.7248444e-02 41:5.3246796e-02 79:4.6455566e-02 98:5.4274429e-02 111:1.0215063e-01 189:9.3750842e-02 230:7.6832324e-02 252:4.2234633e-02 274:5.6942467e-02 276:4.1317280e-02 277:3.8621385e-02 284:3.6869969e-02 296:5.3239051e-02 336:8.6496435e-02 352:1.6604654e-01 354:5.3143345e-02 365:4.1638266e-02 464:3.4826230e-02 480:6.6881403e-02 526:3.4101814e-02 545:5.6270942e-02 550:5.1513255e-02 622:6.7887858e-02 684:6.0539115e-02 755:7.6493047e-02 767:5.8196407e-02 799:8.3615087e-02 802:5.3344302e-02 910:7.2458811e-02 927:6.0960926e-02 929:5.8649912e-02 986:1.6700441e-01 1025:6.9675490e-02 1028:6.8527438e-02 1059:6.1362505e-02 1094:5.9570886e-02 1096:2.0652901e-01 1132:5.2879967e-02 1137:8.0896653e-02 1188:9.6270241e-02 1273:1.4128542e-01 1503:7.1052589e-02 1511:5.9685234e-02 1548:8.8847607e-02 1557:5.6776252e-02 1589:8.0049120e-02 1760:2.2150156e-01 1881:8.1223615e-02 1972:7.4915648e-02 2009:1.0867031e-01 2021:2.8201237e-01 2088:7.3738441e-02 2098:1.7699897e-01 2099:1.7802052e-01 2122:7.5097844e-02 2141:9.2161000e-02 2264:1.1130272e-01 2362:4.8006292e-02 2550:9.7868025e-02 2564:1.2834764e-01 2607:9.5717140e-02 2820:1.3676767e-01 3033:9.1752172e-02 3198:9.4323657e-02 3272:1.0240593e-01 3410:1.0618065e-01 3512:8.5455991e-02 3574:9.5631681e-02 3675:1.1940711e-01 4678:1.2296332e-01 5206:1.1661388e-01 6147:1.8070130e-01 7541:1.3165449e-01 7840:2.9703492e-01 8440:1.2342619e-01 10988:1.2549973e-01 11049:1.4151667e-01 11168:1.6530864e-01 31847:2.9991940e-01\r\n",
      "0 |f 5:5.3301733e-02 9:6.2555432e-02 11:7.3172815e-02 13:4.9635198e-02 15:6.5190420e-02 17:2.8164724e-02 18:3.5669319e-02 19:5.5451479e-02 20:4.1486122e-02 24:1.5185564e-02 26:3.9898835e-02 33:3.7937440e-02 36:6.0122520e-02 45:6.7186847e-02 50:4.8080895e-02 51:5.1502939e-02 52:7.0759192e-02 54:2.6368383e-02 56:5.5773970e-02 61:1.0710372e-01 67:5.0984230e-02 69:3.4223195e-02 74:3.9380819e-02 80:5.8718670e-02 82:1.1237274e-01 85:4.5723863e-02 90:6.8084776e-02 102:6.4724497e-02 107:4.8656568e-02 108:6.8869792e-02 111:5.3116240e-02 112:5.0638448e-02 117:4.5061130e-02 125:8.0561966e-02 144:5.3790558e-02 191:4.7557503e-02 210:4.0814247e-02 215:5.1698022e-02 222:6.0015813e-02 224:3.7807085e-02 231:4.1280117e-02 233:3.0302791e-02 234:6.7837529e-02 255:1.0167062e-01 259:4.0633537e-02 284:5.4960225e-02 286:7.0582502e-02 301:7.8823715e-02 318:5.8683388e-02 327:6.8761639e-02 348:7.2057448e-02 365:8.7477766e-02 370:5.2241467e-02 376:7.7616692e-02 381:4.9030\r\n",
      "gzip: stdin: unexpected end of file\r\n"
     ]
    }
   ],
   "source": [
    "! head rcv1/rcv1.train.vw.gz | gunzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[Label] [Importance] [Base] [Tag]|Namespace Features |Namespace Features ... |Namespace Features\n",
    "```\n",
    "\n",
    "* Label - значение целевой переменной. Если не указывать, объект не будет использоваться в обучении\n",
    "* Importance - вес объекта. Если не указывать, равен 1\n",
    "* Base - \"смещение\" при предсказании - см. residual regression . Если не указывать, равен 0\n",
    "* Tag - пометка объекта. Никак не влияет на процесс обучения, но добавляет семантики и \"читаемости\" данных для человека\n",
    "* Namespace - название области признаков. Используется, чтобы разные по сути фичи с одинаковым названием не пересекались\n",
    "* Features - фичи. Это или пара <название фичи>:<значение> или просто <название фичи>. В последнем случае будет считаться, что значение равно 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = result.vw.bin\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = rcv1/rcv1.train.vw.gz\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0   1.0000   0.0000       50\n",
      "0.511062 0.022124            2            2.0   0.0000   0.1487      103\n",
      "0.260481 0.009900            4            4.0   0.0000   0.0418      134\n",
      "0.237231 0.213981            8            8.0   0.0000   0.1846      145\n",
      "0.247655 0.258079           16           16.0   1.0000   0.2879       23\n",
      "0.242766 0.237877           32           32.0   0.0000   0.1818       31\n",
      "0.236583 0.230399           64           64.0   0.0000   0.1566       60\n",
      "0.229860 0.223137          128          128.0   1.0000   0.8289      105\n",
      "0.186476 0.143092          256          256.0   1.0000   0.8526      122\n",
      "0.156768 0.127060          512          512.0   0.0000   0.4880       74\n",
      "0.128087 0.099406         1024         1024.0   0.0000   0.3153       99\n",
      "0.102213 0.076338         2048         2048.0   1.0000   0.6569       31\n",
      "0.088079 0.073944         4096         4096.0   0.0000   0.2314      114\n",
      "0.077332 0.066586         8192         8192.0   0.0000   0.5016      103\n",
      "0.067194 0.057056        16384        16384.0   1.0000   1.0000       49\n",
      "0.059360 0.051526        32768        32768.0   1.0000   0.9464      103\n",
      "0.053951 0.048541        65536        65536.0   1.0000   1.0000       33\n",
      "0.049552 0.045153       131072       131072.0   1.0000   1.0000       50\n",
      "0.046298 0.043045       262144       262144.0   0.0000   0.0000       49\n",
      "0.043206 0.040114       524288       524288.0   0.0000   0.4751      106\n",
      "\n",
      "finished run\n",
      "number of examples = 781265\n",
      "weighted example sum = 781265.000000\n",
      "weighted label sum = 370541.000000\n",
      "average loss = 0.041728\n",
      "best constant = 0.474283\n",
      "best constant's loss = 0.249339\n",
      "total feature number = 59936409\n",
      "13.57user 2.15system 0:13.00elapsed 120%CPU (0avgtext+0avgdata 19860maxresident)k\n",
      "744inputs+616outputs (4major+1778minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "! time vw rcv1/rcv1.train.vw.gz --final_regressor result.vw.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `-f`, `--final_regressor` - куда сложить результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308K\tresult.vw.bin\r\n"
     ]
    }
   ],
   "source": [
    "! du -h result.vw.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = predictions.txt\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = rcv1/rcv1.test.vw.gz\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0   0.0000   0.0000       40\n",
      "0.000000 0.000000            2            2.0   1.0000   1.0000       74\n",
      "0.000000 0.000000            4            4.0   0.0000   0.0000       43\n",
      "0.000000 0.000000            8            8.0   0.0000   0.0000       41\n",
      "0.007352 0.014703           16           16.0   1.0000   1.0000       33\n",
      "0.006755 0.006159           32           32.0   0.0000   0.0000      209\n",
      "0.017643 0.028531           64           64.0   1.0000   1.0000       24\n",
      "0.019877 0.022112          128          128.0   1.0000   1.0000       44\n",
      "0.066147 0.112417          256          256.0   0.0000   0.0000       48\n",
      "0.056477 0.046807          512          512.0   1.0000   1.0000       32\n",
      "0.046589 0.036701         1024         1024.0   0.0000   0.0000      123\n",
      "0.041715 0.036840         2048         2048.0   1.0000   0.8463       63\n",
      "0.041956 0.042198         4096         4096.0   0.0000   0.0000       98\n",
      "0.042283 0.042609         8192         8192.0   0.0000   0.0000       63\n",
      "0.042623 0.042964        16384        16384.0   0.0000   0.0000      226\n",
      "\n",
      "finished run\n",
      "number of examples = 23149\n",
      "weighted example sum = 23149.000000\n",
      "weighted label sum = 10786.000000\n",
      "average loss = 0.042672\n",
      "best constant = 0.465938\n",
      "best constant's loss = 0.248840\n",
      "total feature number = 1775682\n",
      "0.40user 0.09system 0:00.38elapsed 131%CPU (0avgtext+0avgdata 16512maxresident)k\n",
      "0inputs+176outputs (0major+1519minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "! time vw --testonly --initial_regressor result.vw.bin --predictions predictions.txt rcv1/rcv1.test.vw.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `-t`, `--testonly` - не обучать ничего, только предикты\n",
    "* `-i`, `--initial_regressor` - использовать начальные веса из файла\n",
    "* `-p`, `--predictions` - куда сложить предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "1\r\n",
      "0\r\n",
      "0\r\n",
      "0\r\n",
      "0\r\n",
      "0\r\n",
      "0\r\n",
      "0\r\n",
      "0.318022\r\n"
     ]
    }
   ],
   "source": [
    "! head predictions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Категориальные фичи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Категориальные фичи предполагается кодировать различными бинарными признаками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"\"\"cat <<EOT >> cat_feat.vw\n",
    "1 | color_1 color_2 enabled\n",
    "1 | color_3 endabled\n",
    "0 | color_1 disabled\n",
    "1 | color_2 color_3 disabled\n",
    "0 | color_2 disabled\n",
    "EOT\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | color_1 color_2 enabled\r\n",
      "1 | color_3 endabled\r\n",
      "0 | color_1 disabled\r\n",
      "1 | color_2 color_3 disabled\r\n",
      "0 | color_2 disabled\r\n"
     ]
    }
   ],
   "source": [
    "! cat cat_feat.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = cat_result.vw.bin\r\n",
      "Num weight bits = 18\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = cat_feat.vw\r\n",
      "num sources = 1\r\n",
      "average  since         example        example  current  current  current\r\n",
      "loss     last          counter         weight    label  predict features\r\n",
      "1.000000 1.000000            1            1.0   1.0000   0.0000        4\r\n",
      "0.854476 0.708951            2            2.0   1.0000   0.1580        3\r\n",
      "0.565922 0.277369            4            4.0   1.0000   0.3936        4\r\n",
      "\r\n",
      "finished run\r\n",
      "number of examples = 5\r\n",
      "weighted example sum = 5.000000\r\n",
      "weighted label sum = 3.000000\r\n",
      "average loss = 0.503381\r\n",
      "best constant = 0.600000\r\n",
      "best constant's loss = 0.240000\r\n",
      "total feature number = 17\r\n"
     ]
    }
   ],
   "source": [
    "! vw cat_feat.vw -f cat_result.vw.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\r\n",
      "predictions = cat_preds.txt\r\n",
      "Num weight bits = 18\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = cat_feat.vw\r\n",
      "num sources = 1\r\n",
      "average  since         example        example  current  current  current\r\n",
      "loss     last          counter         weight    label  predict features\r\n",
      "0.126729 0.126729            1            1.0   1.0000   0.6440        4\r\n",
      "0.112434 0.098139            2            2.0   1.0000   0.6867        3\r\n",
      "0.118843 0.125252            4            4.0   1.0000   0.5322        4\r\n",
      "\r\n",
      "finished run\r\n",
      "number of examples = 5\r\n",
      "weighted example sum = 5.000000\r\n",
      "weighted label sum = 3.000000\r\n",
      "average loss = 0.107927\r\n",
      "best constant = 0.600000\r\n",
      "best constant's loss = 0.240000\r\n",
      "total feature number = 17\r\n"
     ]
    }
   ],
   "source": [
    "! vw -t -i cat_result.vw.bin -p cat_preds.txt cat_feat.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.644010\r\n",
      "0.686728\r\n",
      "0.177915\r\n",
      "0.532185\r\n",
      "0.253504\r\n"
     ]
    }
   ],
   "source": [
    "! cat cat_preds.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Докрутки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = \r\n",
      "num sources = 1\r\n",
      "driver:\r\n",
      "  --onethread           Disable parse thread\r\n",
      "VW options:\r\n",
      "  --ring_size arg (=256, ) size of example ring\r\n",
      "  --strict_parse           throw on malformed examples\r\n",
      "Update options:\r\n",
      "  -l [ --learning_rate ] arg Set learning rate\r\n",
      "  --power_t arg              t power value\r\n",
      "  --decay_learning_rate arg  Set Decay factor for learning_rate between passes\r\n",
      "  --initial_t arg            initial t value\r\n",
      "  --feature_mask arg         Use existing regressor to determine which \r\n",
      "                             parameters may be updated.  If no \r\n",
      "                             initial_regressor given, also used for initial \r\n",
      "                             weights.\r\n",
      "Weight options:\r\n",
      "  -i [ --initial_regressor ] arg  Initial regressor(s)\r\n",
      "  --initial_weight arg            Set all weights to an initial value of arg.\r\n",
      "  --random_weights                make initial weights random\r\n",
      "  --normal_weights                make initial weights normal\r\n",
      "  --truncated_normal_weights      make initial weights truncated normal\r\n",
      "  --sparse_weights                Use a sparse datastructure for weights\r\n",
      "  --input_feature_regularizer arg Per feature regularization input file\r\n",
      "Parallelization options:\r\n",
      "  --span_server arg                 Location of server for setting up spanning \r\n",
      "                                    tree\r\n",
      "  --unique_id arg (=0, )            unique id used for cluster parallel jobs\r\n",
      "  --total arg (=1, )                total number of nodes used in cluster \r\n",
      "                                    parallel job\r\n",
      "  --node arg (=0, )                 node number in cluster parallel job\r\n",
      "  --span_server_port arg (=26543, ) Port of the server for setting up spanning \r\n",
      "                                    tree\r\n",
      "Diagnostic options:\r\n",
      "  --version             Version information\r\n",
      "  -a [ --audit ]        print weights of features\r\n",
      "  -P [ --progress ] arg Progress update frequency. int: additive, float: \r\n",
      "                        multiplicative\r\n",
      "  --quiet               Don't output disgnostics and progress updates\r\n",
      "  -h [ --help ]         Look here: http://hunch.net/~vw/ and click on Tutorial.\r\n",
      "Randomization options:\r\n",
      "  --random_seed arg     seed random number generator\r\n",
      "Feature options:\r\n",
      "  --hash arg                      how to hash the features. Available options: \r\n",
      "                                  strings, all\r\n",
      "  --hash_seed arg (=0, )          seed for hash function\r\n",
      "  --ignore arg                    ignore namespaces beginning with character \r\n",
      "                                  <arg>\r\n",
      "  --ignore_linear arg             ignore namespaces beginning with character \r\n",
      "                                  <arg> for linear terms only\r\n",
      "  --keep arg                      keep namespaces beginning with character \r\n",
      "                                  <arg>\r\n",
      "  --redefine arg                  redefine namespaces beginning with characters\r\n",
      "                                  of std::string S as namespace N. <arg> shall \r\n",
      "                                  be in form 'N:=S' where := is operator. Empty\r\n",
      "                                  N or S are treated as default namespace. Use \r\n",
      "                                  ':' as a wildcard in S.\r\n",
      "  -b [ --bit_precision ] arg      number of bits in the feature table\r\n",
      "  --noconstant                    Don't add a constant feature\r\n",
      "  -C [ --constant ] arg           Set initial value of constant\r\n",
      "  --ngram arg                     Generate N grams. To generate N grams for a \r\n",
      "                                  single namespace 'foo', arg should be fN.\r\n",
      "  --skips arg                     Generate skips in N grams. This in \r\n",
      "                                  conjunction with the ngram tag can be used to\r\n",
      "                                  generate generalized n-skip-k-gram. To \r\n",
      "                                  generate n-skips for a single namespace \r\n",
      "                                  'foo', arg should be fN.\r\n",
      "  --feature_limit arg             limit to N features. To apply to a single \r\n",
      "                                  namespace 'foo', arg should be fN\r\n",
      "  --affix arg                     generate prefixes/suffixes of features; \r\n",
      "                                  argument '+2a,-3b,+1' means generate 2-char \r\n",
      "                                  prefixes for namespace a, 3-char suffixes for\r\n",
      "                                  b and 1 char prefixes for default namespace\r\n",
      "  --spelling arg                  compute spelling features for a give \r\n",
      "                                  namespace (use '_' for default namespace)\r\n",
      "  --dictionary arg                read a dictionary for additional features \r\n",
      "                                  (arg either 'x:file' or just 'file')\r\n",
      "  --dictionary_path arg           look in this directory for dictionaries; \r\n",
      "                                  defaults to current directory or env{PATH}\r\n",
      "  --interactions arg              Create feature interactions of any level \r\n",
      "                                  between namespaces.\r\n",
      "  --permutations                  Use permutations instead of combinations for \r\n",
      "                                  feature interactions of same namespace.\r\n",
      "  --leave_duplicate_interactions  Don't remove interactions with duplicate \r\n",
      "                                  combinations of namespaces. For ex. this is a\r\n",
      "                                  duplicate: '-q ab -q ba' and a lot more in \r\n",
      "                                  '-q ::'.\r\n",
      "  -q [ --quadratic ] arg          Create and use quadratic features\r\n",
      "  --q: arg                        : corresponds to a wildcard for all printable\r\n",
      "                                  characters\r\n",
      "  --cubic arg                     Create and use cubic features\r\n",
      "Example options:\r\n",
      "  -t [ --testonly ]                Ignore label information and just test\r\n",
      "  --holdout_off                    no holdout data in multiple passes\r\n",
      "  --holdout_period arg (=10, )     holdout period for test only\r\n",
      "  --holdout_after arg              holdout after n training examples, default \r\n",
      "                                   off (disables holdout_period)\r\n",
      "  --early_terminate arg (=3, )     Specify the number of passes tolerated when \r\n",
      "                                   holdout loss doesn't decrease before early \r\n",
      "                                   termination\r\n",
      "  --passes arg                     Number of Training Passes\r\n",
      "  --initial_pass_length arg        initial number of examples per pass\r\n",
      "  --examples arg                   number of examples to parse\r\n",
      "  --min_prediction arg             Smallest prediction to output\r\n",
      "  --max_prediction arg             Largest prediction to output\r\n",
      "  --sort_features                  turn this on to disregard order in which \r\n",
      "                                   features have been defined. This will lead \r\n",
      "                                   to smaller cache sizes\r\n",
      "  --loss_function arg (=squared, ) Specify the loss function to be used, uses \r\n",
      "                                   squared by default. Currently available ones\r\n",
      "                                   are squared, classic, hinge, logistic, \r\n",
      "                                   quantile and poisson.\r\n",
      "  --quantile_tau arg (=0.5, )      Parameter \\tau associated with Quantile \r\n",
      "                                   loss. Defaults to 0.5\r\n",
      "  --l1 arg                         l_1 lambda\r\n",
      "  --l2 arg                         l_2 lambda\r\n",
      "  --no_bias_regularization         no bias in regularization\r\n",
      "  --named_labels arg               use names for labels (multiclass, etc.) \r\n",
      "                                   rather than integers, argument specified all\r\n",
      "                                   possible labels, comma-sep, eg \r\n",
      "                                   \"--named_labels Noun,Verb,Adj,Punc\"\r\n",
      "Output model:\r\n",
      "  -f [ --final_regressor ] arg          Final regressor\r\n",
      "  --readable_model arg                  Output human-readable final regressor \r\n",
      "                                        with numeric features\r\n",
      "  --invert_hash arg                     Output human-readable final regressor \r\n",
      "                                        with feature names.  Computationally \r\n",
      "                                        expensive.\r\n",
      "  --save_resume                         save extra state so learning can be \r\n",
      "                                        resumed later with new data\r\n",
      "  --preserve_performance_counters       reset performance counters when \r\n",
      "                                        warmstarting\r\n",
      "  --save_per_pass                       Save the model after every pass over \r\n",
      "                                        data\r\n",
      "  --output_feature_regularizer_binary arg\r\n",
      "                                        Per feature regularization output file\r\n",
      "  --output_feature_regularizer_text arg Per feature regularization output file,\r\n",
      "                                        in text\r\n",
      "  --id arg                              User supplied ID embedded into the \r\n",
      "                                        final regressor\r\n",
      "Output options:\r\n",
      "  -p [ --predictions ] arg     File to output predictions to\r\n",
      "  -r [ --raw_predictions ] arg File to output unnormalized predictions to\r\n",
      "Audit Regressor:\r\n",
      "  --audit_regressor arg stores feature names and their regressor values. Same \r\n",
      "                        dataset must be used for both regressor training and \r\n",
      "                        this mode.\r\n",
      "Search options:\r\n",
      "  --search arg                          Use learning to search, \r\n",
      "                                        argument=maximum action id or 0 for LDF\r\n",
      "  --search_task arg                     the search task (use \"--search_task \r\n",
      "                                        list\" to get a list of available tasks)\r\n",
      "  --search_metatask arg                 the search metatask (use \r\n",
      "                                        \"--search_metatask list\" to get a list \r\n",
      "                                        of available metatasks)\r\n",
      "  --search_interpolation arg            at what level should interpolation \r\n",
      "                                        happen? [*data|policy]\r\n",
      "  --search_rollout arg                  how should rollouts be executed?       \r\n",
      "                                            [policy|oracle|*mix_per_state|mix_p\r\n",
      "                                        er_roll|none]\r\n",
      "  --search_rollin arg                   how should past trajectories be \r\n",
      "                                        generated? [policy|oracle|*mix_per_stat\r\n",
      "                                        e|mix_per_roll]\r\n",
      "  --search_passes_per_policy arg (=1, ) number of passes per policy (only valid\r\n",
      "                                        for search_interpolation=policy)\r\n",
      "  --search_beta arg (=0.5, )            interpolation rate for policies (only \r\n",
      "                                        valid for search_interpolation=policy)\r\n",
      "  --search_alpha arg (=1e-10, )         annealed beta = 1-(1-alpha)^t (only \r\n",
      "                                        valid for search_interpolation=data)\r\n",
      "  --search_total_nb_policies arg        if we are going to train the policies \r\n",
      "                                        through multiple separate calls to vw, \r\n",
      "                                        we need to specify this parameter and \r\n",
      "                                        tell vw how many policies are \r\n",
      "                                        eventually going to be trained\r\n",
      "  --search_trained_nb_policies arg      the number of trained policies in a \r\n",
      "                                        file\r\n",
      "  --search_allowed_transitions arg      read file of allowed transitions [def: \r\n",
      "                                        all transitions are allowed]\r\n",
      "  --search_subsample_time arg           instead of training at all timesteps, \r\n",
      "                                        use a subset. if value in (0,1), train \r\n",
      "                                        on a random v%. if v>=1, train on \r\n",
      "                                        precisely v steps per example, if \r\n",
      "                                        v<=-1, use active learning\r\n",
      "  --search_neighbor_features arg        copy features from neighboring lines. \r\n",
      "                                        argument looks like: '-1:a,+2' meaning \r\n",
      "                                        copy previous line namespace a and next\r\n",
      "                                        next line from namespace _unnamed_, \r\n",
      "                                        where ',' separates them\r\n",
      "  --search_rollout_num_steps arg        how many calls of \"loss\" before we stop\r\n",
      "                                        really predicting on rollouts and \r\n",
      "                                        switch to oracle (default means \r\n",
      "                                        \"infinite\")\r\n",
      "  --search_history_length arg (=1, )    some tasks allow you to specify how \r\n",
      "                                        much history their depend on; specify \r\n",
      "                                        that here\r\n",
      "  --search_no_caching                   turn off the built-in caching ability \r\n",
      "                                        (makes things slower, but technically \r\n",
      "                                        more safe)\r\n",
      "  --search_xv                           train two separate policies, \r\n",
      "                                        alternating prediction/learning\r\n",
      "  --search_perturb_oracle arg (=0, )    perturb the oracle on rollin with this \r\n",
      "                                        probability\r\n",
      "  --search_linear_ordering              insist on generating examples in linear\r\n",
      "                                        order (def: hoopla permutation)\r\n",
      "  --search_active_verify arg            verify that active learning is doing \r\n",
      "                                        the right thing (arg = multiplier, \r\n",
      "                                        should be = cost_range * range_c)\r\n",
      "  --search_save_every_k_runs arg        save model every k runs\r\n",
      "Experience Replay:\r\n",
      "  --replay_c arg              use experience replay at a specified level \r\n",
      "                              [b=classification/regression, m=multiclass, \r\n",
      "                              c=cost sensitive] with specified buffer size\r\n",
      "  --replay_c_count arg (=1, ) how many times (in expectation) should each \r\n",
      "                              example be played (default: 1 = permuting)\r\n",
      "Explore evaluation:\r\n",
      "  --explore_eval        Evaluate explore_eval adf policies\r\n",
      "  --multiplier arg      Multiplier used to make all rejection sample \r\n",
      "                        probabilities <= 1\r\n",
      "Make csoaa_ldf into Contextual Bandit:\r\n",
      "  --cbify_ldf           Convert csoaa_ldf into a contextual bandit problem\r\n",
      "  --loss0 arg (=0, )    loss for correct label\r\n",
      "  --loss1 arg (=1, )    loss for incorrect label\r\n",
      "Make Multiclass into Contextual Bandit:\r\n",
      "  --cbify arg           Convert multiclass on <k> classes into a contextual \r\n",
      "                        bandit problem\r\n",
      "  --cbify_cs            consume cost-sensitive classification examples instead \r\n",
      "                        of multiclass\r\n",
      "  --loss0 arg (=0, )    loss for correct label\r\n",
      "  --loss1 arg (=1, )    loss for incorrect label\r\n",
      "Make Multiclass into Warm-starting Contextual Bandit:\r\n",
      "  --warm_cb arg                        Convert multiclass on <k> classes into a\r\n",
      "                                       contextual bandit problem\r\n",
      "  --warm_cb_cs                         consume cost-sensitive classification \r\n",
      "                                       examples instead of multiclass\r\n",
      "  --loss0 arg (=0, )                   loss for correct label\r\n",
      "  --loss1 arg (=1, )                   loss for incorrect label\r\n",
      "  --warm_start arg (=0, )              number of training examples for warm \r\n",
      "                                       start phase\r\n",
      "  --epsilon arg                        epsilon-greedy exploration\r\n",
      "  --interaction arg (=4294967295, )    number of examples for the interactive \r\n",
      "                                       contextual bandit learning phase\r\n",
      "  --warm_start_update                  indicator of warm start updates\r\n",
      "  --interaction_update                 indicator of interaction updates\r\n",
      "  --corrupt_type_warm_start arg (=1, ) type of label corruption in the warm \r\n",
      "                                       start phase (1: uniformly at random, 2: \r\n",
      "                                       circular, 3: replacing with overwriting \r\n",
      "                                       label)\r\n",
      "  --corrupt_prob_warm_start arg (=0, ) probability of label corruption in the \r\n",
      "                                       warm start phase\r\n",
      "  --choices_lambda arg (=1, )          the number of candidate lambdas to \r\n",
      "                                       aggregate (lambda is the importance \r\n",
      "                                       weight parameter between the two \r\n",
      "                                       sources)\r\n",
      "  --lambda_scheme arg (=1, )           The scheme for generating candidate \r\n",
      "                                       lambda set (1: center lambda=0.5, 2: \r\n",
      "                                       center lambda=0.5, min lambda=0, max \r\n",
      "                                       lambda=1, 3: center lambda=epsilon/(1+ep\r\n",
      "                                       silon), 4: center lambda=epsilon/(1+epsi\r\n",
      "                                       lon), min lambda=0, max lambda=1); the \r\n",
      "                                       rest of candidate lambda values are \r\n",
      "                                       generated using a doubling scheme\r\n",
      "  --overwrite_label arg (=1, )         the label used by type 3 corruptions \r\n",
      "                                       (overwriting)\r\n",
      "  --sim_bandit                         simulate contextual bandit updates on \r\n",
      "                                       warm start examples\r\n",
      "EXPERIMENTAL: Conditional Contextual Bandit Exploration with Action Dependent Features:\r\n",
      "  --ccb_explore_adf     EXPERIMENTAL: Do Conditional Contextual Bandit learning\r\n",
      "                        with multiline action dependent features.\r\n",
      "  --slate               EXPERIMENTAL - MAY CHANGE: Enable slate mode in CCB.\r\n",
      "CB Sample:\r\n",
      "  --cb_sample           Sample from CB pdf and swap top action.\r\n",
      "Contextual Bandit Exploration with Action Dependent Features:\r\n",
      "  --cb_explore_adf      Online explore-exploit for a contextual bandit problem \r\n",
      "                        with multiline action dependent features\r\n",
      "  --epsilon arg         epsilon-greedy exploration\r\n",
      "  --bag arg             bagging-based exploration\r\n",
      "  --greedify            always update first policy once in bagging\r\n",
      "  --first_only          Only explore the first action in a tie-breaking event\r\n",
      "Contextual Bandit Exploration with Action Dependent Features:\r\n",
      "  --cb_explore_adf      Online explore-exploit for a contextual bandit problem \r\n",
      "                        with multiline action dependent features\r\n",
      "  --cover arg           Online cover based exploration\r\n",
      "  --psi arg (=1, )      disagreement parameter for cover\r\n",
      "  --nounif              do not explore uniformly on zero-probability actions in\r\n",
      "                        cover\r\n",
      "  --first_only          Only explore the first action in a tie-breaking event\r\n",
      "  --cb_type arg         contextual bandit method to use in {ips,dr,mtr}. \r\n",
      "                        Default: mtr\r\n",
      "Contextual Bandit Exploration with Action Dependent Features:\r\n",
      "  --cb_explore_adf      Online explore-exploit for a contextual bandit problem \r\n",
      "                        with multiline action dependent features\r\n",
      "  --first arg           tau-first exploration\r\n",
      "  --epsilon arg         epsilon-greedy exploration\r\n",
      "Contextual Bandit Exploration with Action Dependent Features:\r\n",
      "  --cb_explore_adf          Online explore-exploit for a contextual bandit \r\n",
      "                            problem with multiline action dependent features\r\n",
      "  --regcb                   RegCB-elim exploration\r\n",
      "  --regcbopt                RegCB optimistic exploration\r\n",
      "  --mellowness arg (=0.1, ) RegCB mellowness parameter c_0. Default 0.1\r\n",
      "  --cb_min_cost arg (=0, )  lower bound on cost\r\n",
      "  --cb_max_cost arg (=1, )  upper bound on cost\r\n",
      "  --first_only              Only explore the first action in a tie-breaking \r\n",
      "                            event\r\n",
      "  --cb_type arg             contextual bandit method to use in {ips,dr,mtr}. \r\n",
      "                            Default: mtr\r\n",
      "Contextual Bandit Exploration with Action Dependent Features:\r\n",
      "  --cb_explore_adf      Online explore-exploit for a contextual bandit problem \r\n",
      "                        with multiline action dependent features\r\n",
      "  --epsilon arg         epsilon-greedy exploration\r\n",
      "  --softmax             softmax exploration\r\n",
      "  --lambda arg (=1, )   parameter for softmax\r\n",
      "Contextual Bandit Exploration with Action Dependent Features:\r\n",
      "  --cb_explore_adf      Online explore-exploit for a contextual bandit problem \r\n",
      "                        with multiline action dependent features\r\n",
      "  --epsilon arg         epsilon-greedy exploration\r\n",
      "  --first_only          Only explore the first action in a tie-breaking event\r\n",
      "Contextual Bandit Exploration:\r\n",
      "  --cb_explore arg        Online explore-exploit for a <k> action contextual \r\n",
      "                          bandit problem\r\n",
      "  --first arg             tau-first exploration\r\n",
      "  --epsilon arg (=0.05, ) epsilon-greedy exploration\r\n",
      "  --bag arg               bagging-based exploration\r\n",
      "  --cover arg             Online cover based exploration\r\n",
      "  --psi arg (=1, )        disagreement parameter for cover\r\n",
      "Multiworld Testing Options:\r\n",
      "  --multiworld_test arg Evaluate features as a policies\r\n",
      "  --learn arg           Do Contextual Bandit learning on <n> classes.\r\n",
      "  --exclude_eval        Discard mwt policy features before learning\r\n",
      "Contextual Bandit with Action Dependent Features:\r\n",
      "  --cb_adf              Do Contextual Bandit learning with multiline action \r\n",
      "                        dependent features.\r\n",
      "  --rank_all            Return actions sorted by score order\r\n",
      "  --no_predict          Do not do a prediction when training\r\n",
      "  --clip_p arg (=0, )   Clipping probability in importance weight. Default: 0.f\r\n",
      "                        (no clipping).\r\n",
      "  --cb_type arg         contextual bandit method to use in {ips, dm, dr, mtr, \r\n",
      "                        sm}. Default: mtr\r\n",
      "Contextual Bandit Options:\r\n",
      "  --cb arg              Use contextual bandit learning with <k> costs\r\n",
      "  --cb_type arg         contextual bandit method to use in {ips,dm,dr}\r\n",
      "  --eval                Evaluate a policy rather than optimizing.\r\n",
      "Cost Sensitive One Against All with Label Dependent Features:\r\n",
      "  --csoaa_ldf arg       Use one-against-all multiclass learning with label \r\n",
      "                        dependent features.\r\n",
      "  --ldf_override arg    Override singleline or multiline from csoaa_ldf or \r\n",
      "                        wap_ldf, eg if stored in file\r\n",
      "  --csoaa_rank          Return actions sorted by score order\r\n",
      "  --probabilities       predict probabilites of all classes\r\n",
      "Cost Sensitive One Against All with Label Dependent Features:\r\n",
      "  --wap_ldf arg         Use weighted all-pairs multiclass learning with label \r\n",
      "                        dependent features.  Specify singleline or multiline.\r\n",
      "Interact via elementwise multiplication:\r\n",
      "  --interact arg        Put weights on feature products from namespaces <n1> \r\n",
      "                        and <n2>\r\n",
      "Cost Sensitive One Against All:\r\n",
      "  --csoaa arg           One-against-all multiclass with <k> costs\r\n",
      "Cost-sensitive Active Learning:\r\n",
      "  --cs_active arg                       Cost-sensitive active learning with <k>\r\n",
      "                                        costs\r\n",
      "  --simulation                          cost-sensitive active learning \r\n",
      "                                        simulation mode\r\n",
      "  --baseline                            cost-sensitive active learning baseline\r\n",
      "  --domination arg (=1, )               cost-sensitive active learning use \r\n",
      "                                        domination. Default 1\r\n",
      "  --mellowness arg (=0.1, )             mellowness parameter c_0. Default 0.1.\r\n",
      "  --range_c arg (=0.5, )                parameter controlling the threshold for\r\n",
      "                                        per-label cost uncertainty. Default \r\n",
      "                                        0.5.\r\n",
      "  --max_labels arg (=18446744073709551615, )\r\n",
      "                                        maximum number of label queries.\r\n",
      "  --min_labels arg (=18446744073709551615, )\r\n",
      "                                        minimum number of label queries.\r\n",
      "  --cost_max arg (=1, )                 cost upper bound. Default 1.\r\n",
      "  --cost_min arg (=0, )                 cost lower bound. Default 0.\r\n",
      "  --csa_debug                           print debug stuff for cs_active\r\n",
      "Multilabel One Against All:\r\n",
      "  --multilabel_oaa arg  One-against-all multilabel with <k> labels\r\n",
      "importance weight classes:\r\n",
      "  --classweight arg     importance weight multiplier for class\r\n",
      "Memory Tree:\r\n",
      "  --memory_tree arg (=0, )             Make a memory tree with at most <n> \r\n",
      "                                       nodes\r\n",
      "  --max_number_of_labels arg (=10, )   max number of unique label\r\n",
      "  --leaf_example_multiplier arg (=1, ) multiplier on examples per leaf (default\r\n",
      "                                       = log nodes)\r\n",
      "  --alpha arg (=0.1, )                 Alpha\r\n",
      "  --dream_repeats arg (=1, )           number of dream operations per example \r\n",
      "                                       (default = 1)\r\n",
      "  --top_K arg (=1, )                   top K prediction error (default 1)\r\n",
      "  --learn_at_leaf                      whether or not learn at leaf (defualt = \r\n",
      "                                       True)\r\n",
      "  --oas                                use oas at the leaf\r\n",
      "  --dream_at_update arg (=0, )         turn on dream operations at reward based\r\n",
      "                                       update as well\r\n",
      "  --online                             turn on dream operations at reward based\r\n",
      "                                       update as well\r\n",
      "Recall Tree:\r\n",
      "  --recall_tree arg       Use online tree for multiclass\r\n",
      "  --max_candidates arg    maximum number of labels per leaf in the tree\r\n",
      "  --bern_hyper arg (=1, ) recall tree depth penalty\r\n",
      "  --max_depth arg         maximum depth of the tree, default log_2 (#classes)\r\n",
      "  --node_only             only use node features, not full path features\r\n",
      "  --randomized_routing    randomized routing\r\n",
      "Logarithmic Time Multiclass Tree:\r\n",
      "  --log_multi arg              Use online tree for multiclass\r\n",
      "  --no_progress                disable progressive validation\r\n",
      "  --swap_resistance arg (=4, ) disable progressive validation\r\n",
      "  --swap_resistance arg (=4, ) higher = more resistance to swap, default=4\r\n",
      "Error Correcting Tournament Options:\r\n",
      "  --ect arg                Error correcting tournament with <k> labels\r\n",
      "  --error arg (=0, )       errors allowed by ECT\r\n",
      "  --link arg (=identity, ) Specify the link function: identity, logistic, glf1 \r\n",
      "                           or poisson\r\n",
      "Boosting:\r\n",
      "  --boosting arg        Online boosting with <N> weak learners\r\n",
      "  --gamma arg (=0.1, )  weak learner's edge (=0.1), used only by online BBM\r\n",
      "  --alg arg (=BBM, )    specify the boosting algorithm: BBM (default), logistic\r\n",
      "                        (AdaBoost.OL.W), adaptive (AdaBoost.OL)\r\n",
      "One Against All Options:\r\n",
      "  --oaa arg             One-against-all multiclass with <k> labels\r\n",
      "  --oaa_subsample arg   subsample this number of negative examples when \r\n",
      "                        learning\r\n",
      "  --probabilities       predict probabilites of all classes\r\n",
      "  --scores              output raw scores per class\r\n",
      "Top K:\r\n",
      "  --top arg             top k recommendation\r\n",
      "Experience Replay:\r\n",
      "  --replay_m arg              use experience replay at a specified level \r\n",
      "                              [b=classification/regression, m=multiclass, \r\n",
      "                              c=cost sensitive] with specified buffer size\r\n",
      "  --replay_m_count arg (=1, ) how many times (in expectation) should each \r\n",
      "                              example be played (default: 1 = permuting)\r\n",
      "Binary loss:\r\n",
      "  --binary              report loss as binary classification on -1,1\r\n",
      "Bootstrap:\r\n",
      "  --bootstrap arg       k-way bootstrap by online importance resampling\r\n",
      "  --bs_type arg         prediction type {mean,vote}\r\n",
      "scorer options:\r\n",
      "  --link arg (=identity, ) Specify the link function: identity, logistic, glf1 \r\n",
      "                           or poisson\r\n",
      "Stagewise polynomial options:\r\n",
      "  --stage_poly                use stagewise polynomial feature learning\r\n",
      "  --sched_exponent arg (=1, ) exponent controlling quantity of included \r\n",
      "                              features\r\n",
      "  --batch_sz arg (=1000, )    multiplier on batch size before including more \r\n",
      "                              features\r\n",
      "  --batch_sz_no_doubling      batch_sz does not double\r\n",
      "Low Rank Quadratics FA:\r\n",
      "  --lrqfa arg           use low rank quadratic features with field aware \r\n",
      "                        weights\r\n",
      "Low Rank Quadratics:\r\n",
      "  --lrq arg             use low rank quadratic features\r\n",
      "  --lrqdropout          use dropout training for low rank quadratic features\r\n",
      "Autolink:\r\n",
      "  --autolink arg        create link function with polynomial d\r\n",
      "VW options:\r\n",
      "  --marginal arg                   substitute marginal label estimates for ids\r\n",
      "  --initial_denominator arg (=1, ) initial denominator\r\n",
      "  --initial_numerator arg (=0.5, ) initial numerator\r\n",
      "  --compete                        enable competition with marginal features\r\n",
      "  --update_before_learn            update marginal values before learning\r\n",
      "  --unweighted_marginals           ignore importance weights when computing \r\n",
      "                                   marginals\r\n",
      "  --decay arg (=0, )               decay multiplier per event (1e-3 for \r\n",
      "                                   example)\r\n",
      "Matrix Factorization Reduction:\r\n",
      "  --new_mf arg          rank for reduction-based matrix factorization\r\n",
      "Neural Network:\r\n",
      "  --nn arg              Sigmoidal feedforward network with <k> hidden units\r\n",
      "  --inpass              Train or test sigmoidal feedforward network with input \r\n",
      "                        passthrough.\r\n",
      "  --multitask           Share hidden layer across all reduced tasks.\r\n",
      "  --dropout             Train or test sigmoidal feedforward network using \r\n",
      "                        dropout.\r\n",
      "  --meanfield           Train or test sigmoidal feedforward network using mean \r\n",
      "                        field.\r\n",
      "Confidence:\r\n",
      "  --confidence                 Get confidence for binary predictions\r\n",
      "  --confidence_after_training  Confidence after training\r\n",
      "Active Learning with Cover:\r\n",
      "  --active_cover                enable active learning with cover\r\n",
      "  --mellowness arg (=8, )       active learning mellowness parameter c_0. \r\n",
      "                                Default 8.\r\n",
      "  --alpha arg (=1, )            active learning variance upper bound parameter \r\n",
      "                                alpha. Default 1.\r\n",
      "  --beta_scale arg (=3.16228, ) active learning variance upper bound parameter \r\n",
      "                                beta_scale. Default std::sqrt(10).\r\n",
      "  --cover arg (=12, )           cover size. Default 12.\r\n",
      "  --oracular                    Use Oracular-CAL style query or not. Default \r\n",
      "                                false.\r\n",
      "Active Learning:\r\n",
      "  --active                enable active learning\r\n",
      "  --simulation            active learning simulation mode\r\n",
      "  --mellowness arg (=8, ) active learning mellowness parameter c_0. Default 8\r\n",
      "Experience Replay:\r\n",
      "  --replay_b arg              use experience replay at a specified level \r\n",
      "                              [b=classification/regression, m=multiclass, \r\n",
      "                              c=cost sensitive] with specified buffer size\r\n",
      "  --replay_b_count arg (=1, ) how many times (in expectation) should each \r\n",
      "                              example be played (default: 1 = permuting)\r\n",
      "Baseline options:\r\n",
      "  --baseline            Learn an additive baseline (from constant features) and\r\n",
      "                        a residual separately in regression.\r\n",
      "  --lr_multiplier arg   learning rate multiplier for baseline model\r\n",
      "  --global_only         use separate example with only global constant for \r\n",
      "                        baseline predictions\r\n",
      "  --check_enabled       only use baseline when the example contains enabled \r\n",
      "                        flag\r\n",
      "OjaNewton options:\r\n",
      "  --OjaNewton                    Online Newton with Oja's Sketch\r\n",
      "  --sketch_size arg (=10, )      size of sketch\r\n",
      "  --epoch_size arg (=1, )        size of epoch\r\n",
      "  --alpha arg (=1, )             mutiplicative constant for indentiy\r\n",
      "  --alpha_inverse arg            one over alpha, similar to learning rate\r\n",
      "  --learning_rate_cnt arg (=2, ) constant for the learning rate 1/t\r\n",
      "  --normalize arg                normalize the features or not\r\n",
      "  --random_init arg              randomize initialization of Oja or not\r\n",
      "LBFGS and Conjugate Gradient options:\r\n",
      "  --conjugate_gradient  use conjugate gradient based optimization\r\n",
      "LBFGS and Conjugate Gradient options:\r\n",
      "  --bfgs                       use conjugate gradient based optimization\r\n",
      "  --hessian_on                 use second derivative in line search\r\n",
      "  --mem arg (=15, )            memory in bfgs\r\n",
      "  --termination arg (=0.001, ) Termination threshold\r\n",
      "Latent Dirichlet Allocation:\r\n",
      "  --lda arg                    Run lda with <int> topics\r\n",
      "  --lda_alpha arg (=0.1, )     Prior on sparsity of per-document topic weights\r\n",
      "  --lda_rho arg (=0.1, )       Prior on sparsity of topic distributions\r\n",
      "  --lda_D arg (=10000, )       Number of documents\r\n",
      "  --lda_epsilon arg (=0.001, ) Loop convergence threshold\r\n",
      "  --minibatch arg (=1, )       Minibatch size, for LDA\r\n",
      "  --math-mode arg (=0, )       Math mode: simd, accuracy, fast-approx\r\n",
      "  --metrics                    Compute metrics\r\n",
      "Noop Learner:\r\n",
      "  --noop                do no learning\r\n",
      "Print psuedolearner:\r\n",
      "  --print               print examples\r\n",
      "Gradient Descent Matrix Factorization:\r\n",
      "  --rank arg            rank for matrix factorization.\r\n",
      "  --bfgs                Option not supported by this reduction\r\n",
      "  --conjugate_gradient  Option not supported by this reduction\r\n",
      "Network sending:\r\n",
      "  --sendto arg          send examples to <host>\r\n",
      "Stochastic Variance Reduced Gradient:\r\n",
      "  --svrg                  Streaming Stochastic Variance Reduced Gradient\r\n",
      "  --stage_size arg (=1, ) Number of passes per SVRG stage\r\n",
      "Follow the Regularized Leader:\r\n",
      "  --ftrl                FTRL: Follow the Proximal Regularized Leader\r\n",
      "  --coin                Coin betting optimizer\r\n",
      "  --pistol              PiSTOL: Parameter-free STOchastic Learning\r\n",
      "  --ftrl_alpha arg      Learning rate for FTRL optimization\r\n",
      "  --ftrl_beta arg       Learning rate for FTRL optimization\r\n",
      "Kernel SVM:\r\n",
      "  --ksvm                   kernel svm\r\n",
      "  --reprocess arg (=1, )   number of reprocess steps for LASVM\r\n",
      "  --pool_greedy            use greedy selection on mini pools\r\n",
      "  --para_active            do parallel active learning\r\n",
      "  --pool_size arg (=1, )   size of pools for active learning\r\n",
      "  --subsample arg (=1, )   number of items to subsample from the pool\r\n",
      "  --kernel arg (=linear, ) type of kernel (rbf or linear (default))\r\n",
      "  --bandwidth arg (=1, )   bandwidth of rbf kernel\r\n",
      "  --degree arg (=2, )      degree of poly kernel\r\n",
      "Gradient Descent options:\r\n",
      "  --sgd                  use regular stochastic gradient descent update.\r\n",
      "  --adaptive             use adaptive, individual learning rates.\r\n",
      "  --adax                 use adaptive learning rates with x^2 instead of g^2x^2\r\n",
      "  --invariant            use safe/importance aware updates.\r\n",
      "  --normalized           use per feature normalized updates\r\n",
      "  --sparse_l2 arg (=0, ) use per feature normalized updates\r\n",
      "  --l1_state arg (=0, )  use per feature normalized updates\r\n",
      "  --l2_state arg (=1, )  use per feature normalized updates\r\n",
      "Input options:\r\n",
      "  -d [ --data ] arg     Example set\r\n",
      "  --daemon              persistent daemon mode on port 26542\r\n",
      "  --foreground          in persistent daemon mode, do not run in the background\r\n",
      "  --port arg            port to listen on; use 0 to pick unused port\r\n",
      "  --num_children arg    number of children for persistent daemon mode\r\n",
      "  --pid_file arg        Write pid file in persistent daemon mode\r\n",
      "  --port_file arg       Write port used in persistent daemon mode\r\n",
      "  -c [ --cache ]        Use a cache.  The default is <data>.cache\r\n",
      "  --cache_file arg      The location(s) of cache_file.\r\n",
      "  --json                Enable JSON parsing.\r\n",
      "  --dsjson              Enable Decision Service JSON parsing.\r\n",
      "  -k [ --kill_cache ]   do not reuse existing cache: create a new one always\r\n",
      "  --compressed          use gzip format whenever possible. If a cache file is \r\n",
      "                        being created, this option creates a compressed cache \r\n",
      "                        file. A mixture of raw-text & compressed inputs are \r\n",
      "                        supported with autodetection.\r\n",
      "  --no_stdin            do not default to reading from stdin\r\n"
     ]
    }
   ],
   "source": [
    "! vw --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! vw --normal_weights --quadratic ff --loss_function logistic --l2 0.1 rcv1/rcv1.train.vw.gz --final_regressor result2.vw.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Задачи классификации требуют значения целевой переменной из набора {-1, 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat rcv1/rcv1.train.vw.gz | gunzip | sed -e 's/^0/-1/' > logistic.train.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat rcv1/rcv1.test.vw.gz | gunzip | sed -e 's/^0/-1/' > logistic.test.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 |f 13:3.9656971e-02 24:3.4781646e-02 69:4.6296168e-02 85:6.1853945e-02 140:3.2349996e-02 156:1.0290844e-01 175:6.8493910e-02 188:2.8366476e-02 229:7.4871540e-02 230:9.1505975e-02 234:5.4200061e-02 236:4.4855952e-02 238:5.3422898e-02 387:1.4059304e-01 394:7.5131744e-02 433:1.1118756e-01 434:1.2540409e-01 438:6.5452829e-02 465:2.2644201e-01 468:8.5926279e-02 518:1.0214076e-01 534:9.4191484e-02 613:7.0990764e-02 646:8.7701865e-02 660:7.2289191e-02 709:9.0660661e-02 752:1.0580081e-01 757:6.7965068e-02 812:2.2685185e-01 932:6.8250686e-02 1028:4.8203137e-02 1122:1.2381379e-01 1160:1.3038123e-01 1189:7.1542501e-02 1530:9.2655659e-02 1664:6.5160148e-02 1865:8.5823394e-02 2524:1.6407280e-01 2525:1.1528353e-01 2526:9.7131468e-02 2536:5.7415009e-01 2543:1.4978983e-01 2848:1.0446861e-01 3370:9.2423186e-02 3960:1.5554591e-01 7052:1.2632671e-01 16893:1.9762035e-01 24036:3.2674628e-01 24303:2.2660980e-01\r\n",
      "-1 |f 9:8.5609287e-02 14:2.9904654e-02 19:6.1031535e-02 20:2.1757640e-02 24:1.3484491e-02 39:5.0661467e-02 45:2.5001373e-02 49:6.0599580e-02 50:2.5216307e-02 55:4.5040369e-02 64:5.1786009e-02 65:1.2385602e-01 69:5.0108045e-02 70:3.8740762e-02 73:3.9056923e-02 82:8.0505788e-02 90:3.5707459e-02 99:4.7706675e-02 104:5.1884215e-02 105:1.5568805e-01 106:1.3511626e-01 130:1.0544337e-01 133:8.1089266e-02 171:5.9266284e-02 180:7.7435717e-02 217:6.8277337e-02 233:2.6908301e-02 234:3.5577789e-02 254:1.0109196e-01 286:3.6022667e-02 300:1.2054443e-01 305:3.7950054e-02 326:5.6624860e-02 337:6.8930335e-02 348:3.7790950e-02 400:4.4774704e-02 417:4.3467607e-02 434:8.2317248e-02 441:1.1299837e-01 465:9.6445926e-02 476:5.0166391e-02 481:1.0036784e-01 495:1.0148438e-01 497:4.0623948e-02 510:4.2873766e-02 514:4.4061519e-02 518:6.7046829e-02 548:9.7496063e-02 606:4.7255926e-02 655:5.1991425e-02 678:3.3746067e-02 724:3.5048731e-02 759:6.3103504e-02 768:4.6424236e-02 802:2.4630768e-02 820:4.3894887e-02 910:5.6646861e-02 934:7.6288253e-02 995:4.2333681e-02 1011:4.5684557e-02 1091:6.5844811e-02 1100:1.5925008e-01 1288:4.4281408e-02 1321:5.2120164e-02 1340:1.5260276e-01 1574:7.5433277e-02 1629:6.3325211e-02 1654:3.2249656e-02 1712:1.6111535e-01 1796:8.0251180e-02 1930:8.7665550e-02 2031:1.4472182e-01 2036:8.9130148e-02 2039:9.6134968e-02 2277:8.1756182e-02 2330:7.0579961e-02 2334:8.2388259e-02 2343:8.3097421e-02 2344:2.0492174e-01 2348:7.7909611e-02 2360:9.3834393e-02 2362:3.7530366e-02 2376:7.0720568e-02 2493:1.8794763e-01 2495:5.9343126e-02 2520:1.1095246e-01 2949:5.7359278e-02 3370:2.5833043e-01 4523:1.9765969e-01 4525:1.1142892e-01 5307:9.4257712e-02 5401:7.0384808e-02 5593:8.1356630e-02 6093:7.4096188e-02 9217:1.0568235e-01 11017:8.1550762e-02 12301:1.2676764e-01 12332:2.9680410e-01 12338:9.4060794e-02 12339:2.5156361e-01 12340:2.6923507e-01 12341:1.5476021e-01\r\n",
      "-1 |f 13:4.6038497e-02 20:3.8479928e-02 24:4.0378645e-02 55:4.7046758e-02 90:1.0692423e-01 121:7.5637124e-02 147:6.1341532e-02 161:1.2443262e-01 166:9.2344858e-02 179:5.2878667e-02 188:3.2931156e-02 230:6.2741712e-02 236:5.2074093e-02 240:5.4761782e-02 298:1.2427004e-01 315:7.1311563e-02 350:1.3221318e-01 357:1.5712146e-01 368:1.0088409e-01 433:9.1585882e-02 545:7.7802099e-02 558:6.7504369e-02 573:6.1524265e-02 584:1.1028677e-01 631:9.3503900e-02 660:4.9565587e-02 678:5.9682313e-02 691:1.2557535e-01 820:1.6291782e-01 1059:5.0108973e-02 1139:1.4289856e-01 1145:7.3475920e-02 1183:1.0825604e-01 1188:1.0738946e-01 1272:1.2391161e-01 1335:1.0124054e-01 1529:2.1344736e-01 1910:1.0896006e-01 2066:1.1121049e-01 2090:9.1144264e-02 2162:1.7347734e-01 2812:1.1876234e-01 2841:1.0789524e-01 2871:1.1722078e-01 2878:2.2459558e-01 2903:1.2972572e-01 3006:3.1611764e-01 3008:2.2492266e-01 3394:1.1831106e-01 3762:1.6104744e-01 3845:1.1758391e-01 3852:3.2081831e-01 5118:3.1833124e-01 5651:1.3049011e-01 5656:2.2692566e-01\r\n",
      "-1 |f 3:4.5577556e-02 24:1.1411044e-02 25:3.7974268e-02 33:2.8507719e-02 41:3.5226457e-02 42:4.4543922e-02 50:3.6129922e-02 52:3.1403832e-02 55:2.2511169e-02 69:1.5188689e-02 76:4.0629715e-02 84:7.4837752e-02 85:3.4358751e-02 89:6.5839954e-02 110:2.9084764e-02 161:7.4031636e-02 170:3.5370197e-02 196:4.8320368e-02 208:3.9144717e-02 209:5.5873640e-02 269:4.3558568e-02 276:7.6310657e-02 280:3.7778754e-02 286:3.0483631e-02 288:2.4288468e-02 293:2.3107700e-02 298:3.5118837e-02 314:1.8277630e-01 319:3.1361207e-02 334:6.3679650e-02 336:3.3797063e-02 354:3.5158016e-02 357:4.4402681e-02 389:4.5763917e-02 428:7.6703973e-02 430:6.6179007e-02 433:2.5882259e-02 464:2.3039971e-02 475:5.8085274e-02 476:4.2452544e-02 483:6.5434702e-02 494:3.7052151e-02 533:2.2281876e-02 575:3.5521347e-02 664:4.0118020e-02 678:2.8557094e-02 690:4.1161560e-02 757:3.7753370e-02 802:4.3742239e-02 803:4.1759558e-02 814:6.5572858e-02 836:4.2416338e-02 857:5.1457152e-02 871:4.5915388e-02 963:3.9394960e-02 973:9.8459624e-02 1029:3.6264464e-02 1059:4.0595561e-02 1094:3.9410282e-02 1110:4.2543653e-02 1122:2.8821396e-02 1141:5.7815406e-02 1173:3.5140518e-02 1188:3.0348364e-02 1213:4.0825505e-02 1237:4.2819086e-02 1266:1.1101459e-01 1308:4.8654899e-02 1342:8.2725197e-02 1417:7.9656065e-02 1419:8.4478162e-02 1422:3.3052005e-02 1434:4.7695637e-02 1459:5.6947891e-02 1461:6.3311882e-02 1465:9.5394187e-02 1488:5.8578175e-02 1623:3.8236793e-02 1723:5.8299869e-02 1730:5.7023022e-02 1881:5.3735066e-02 2028:4.5234472e-02 2223:1.0081239e-01 2234:4.1061066e-02 3013:1.0455123e-01 3015:1.2765741e-01 3137:4.5956153e-02 3188:7.3665448e-02 3291:5.2890804e-02 3298:5.7951961e-02 3380:1.0363416e-01 3384:2.0652667e-01 3386:1.1103135e-01 3390:6.5076917e-02 3410:1.4741886e-01 3474:7.3495716e-02 3483:1.8200213e-01 3533:9.9155381e-02 3562:2.3795952e-01 3615:6.1804771e-02 3899:6.2048983e-02 4003:6.2531702e-02 4038:9.2995450e-02 4194:6.7665890e-02 4206:5.8146089e-02 4345:6.6447310e-02 4724:7.4850418e-02 4852:7.5302698e-02 5651:6.2437568e-02 5671:1.2755182e-01 5845:1.3229835e-01 6047:6.8817139e-02 6319:6.4124115e-02 8277:9.3362913e-02 8318:1.6047120e-01 8408:1.1899689e-01 8472:8.1823200e-02 8651:1.1915996e-01 9041:1.1209033e-01 9095:7.3802844e-02 9251:9.2515543e-02 9252:9.7969875e-02 12632:8.0214806e-02 18535:8.5983023e-02 18618:1.1447286e-01 18778:2.0379353e-01 22746:9.7772084e-02 31213:1.1904188e-01 31214:2.0987929e-01 36203:1.3161305e-01 36354:2.9326519e-01 36831:1.7453752e-01 36832:2.1526906e-01\r\n",
      "-1 |f 23:5.8031932e-02 42:4.3960683e-02 45:2.0880008e-02 50:3.5656851e-02 52:3.0992642e-02 69:1.4989815e-02 73:1.9265046e-02 101:3.6552731e-02 131:8.4322646e-02 161:2.7999246e-02 169:3.0079322e-02 179:5.2403107e-02 182:6.2946759e-02 188:1.5550749e-02 193:3.1950910e-02 225:3.7829999e-02 233:2.2472586e-02 236:5.1605769e-02 269:2.5389541e-02 276:2.6976349e-02 298:5.8682792e-02 301:9.8974206e-02 312:3.7751961e-02 342:7.9125486e-02 378:4.9176287e-02 464:2.2738295e-02 478:3.8368121e-02 510:3.5806216e-02 516:3.9175954e-02 533:2.1990126e-02 557:5.4702684e-02 564:4.6411715e-02 642:8.1580848e-02 655:4.3420870e-02 673:3.8477339e-02 702:3.0201910e-02 751:4.8421662e-02 760:5.0214007e-02 768:3.8771410e-02 789:4.6299059e-02 800:6.4234152e-02 802:4.3169495e-02 836:4.1860957e-02 973:5.7390418e-02 1011:3.8153660e-02 1036:6.7988560e-02 1038:4.4288885e-02 1059:4.0064018e-02 1062:4.4851292e-02 1081:9.2638679e-02 1110:4.1986603e-02 1118:5.2589338e-02 1130:4.8003461e-02 1133:4.9178164e-02 1196:4.0469117e-02 1214:5.3173710e-02 1221:5.0143369e-02 1263:8.3388008e-02 1298:1.0334191e-01 1305:4.2508319e-02 1335:4.7807802e-02 1350:5.0633088e-02 1380:3.8872030e-02 1389:4.8115946e-02 1416:4.7130316e-02 1417:4.6430152e-02 1454:3.2705292e-02 1455:4.2858150e-02 1479:5.2930076e-02 1690:5.0393224e-02 1705:6.8371922e-02 1865:7.9661109e-02 1890:4.8129275e-02 2024:4.9730893e-02 2028:7.5585797e-02 2115:6.9944449e-02 2126:4.7789685e-02 2166:5.4787014e-02 2248:4.6545438e-02 2259:8.2881287e-02 2323:5.9373714e-02 2362:3.1343650e-02 2380:6.3111462e-02 2433:5.1356785e-02 2477:8.3841749e-02 2538:5.1214628e-02 2563:6.2386304e-02 2614:4.9308911e-02 3008:4.4509582e-02 3027:1.0765981e-01 3236:1.0831050e-01 3536:7.3078923e-02 3577:9.9980108e-02 3800:5.7047315e-02 3825:1.1666410e-01 3845:5.5525463e-02 4045:1.0749368e-01 4594:9.7434357e-02 4599:8.8552117e-02 5158:6.0078427e-02 5313:8.2510263e-02 5597:1.4109479e-01 6051:1.6858204e-01 6578:7.9471059e-02 7351:6.5885052e-02 8081:7.5523540e-02 8256:2.2026148e-01 8269:8.3418600e-02 8272:8.8049397e-02 8273:1.6269000e-01 8284:2.7112472e-01 8287:2.3736267e-01 8395:9.2970602e-02 8421:8.1172608e-02 8467:8.3752871e-02 8615:7.7104934e-02 10752:1.6747326e-01 13810:1.9023852e-01 14387:8.6804882e-02 18535:8.4857196e-02 18656:1.0005520e-01 18661:3.9166561e-01 18665:2.0358166e-01 18666:7.8335546e-02 18668:1.0148206e-01 20228:1.0928469e-01 24559:1.0928469e-01 25081:8.0901086e-02 33705:1.2259024e-01\r\n",
      "1 |f 5:1.1789641e-01 39:6.0373064e-02 45:1.3163488e-01 60:1.4378849e-01 69:8.6419873e-02 73:4.6543971e-02 140:4.2846322e-02 217:6.5645427e-02 218:6.7507431e-02 232:2.0380184e-01 257:6.6671841e-02 286:4.2928066e-02 288:9.8053738e-02 388:1.3288118e-01 400:2.6614136e-01 522:2.2009864e-01 588:1.8588908e-01 613:1.9352522e-01 655:6.1957959e-02 724:7.0718400e-02 842:5.3571172e-02 911:1.6738084e-01 1012:2.6197922e-01 1115:3.3340582e-01 1239:1.1640005e-01 2937:2.5619447e-01 4003:3.1289726e-01 10731:3.5240188e-01 12773:3.7005949e-01\r\n",
      "-1 |f 15:3.5283413e-02 24:1.7248444e-02 41:5.3246796e-02 79:4.6455566e-02 98:5.4274429e-02 111:1.0215063e-01 189:9.3750842e-02 230:7.6832324e-02 252:4.2234633e-02 274:5.6942467e-02 276:4.1317280e-02 277:3.8621385e-02 284:3.6869969e-02 296:5.3239051e-02 336:8.6496435e-02 352:1.6604654e-01 354:5.3143345e-02 365:4.1638266e-02 464:3.4826230e-02 480:6.6881403e-02 526:3.4101814e-02 545:5.6270942e-02 550:5.1513255e-02 622:6.7887858e-02 684:6.0539115e-02 755:7.6493047e-02 767:5.8196407e-02 799:8.3615087e-02 802:5.3344302e-02 910:7.2458811e-02 927:6.0960926e-02 929:5.8649912e-02 986:1.6700441e-01 1025:6.9675490e-02 1028:6.8527438e-02 1059:6.1362505e-02 1094:5.9570886e-02 1096:2.0652901e-01 1132:5.2879967e-02 1137:8.0896653e-02 1188:9.6270241e-02 1273:1.4128542e-01 1503:7.1052589e-02 1511:5.9685234e-02 1548:8.8847607e-02 1557:5.6776252e-02 1589:8.0049120e-02 1760:2.2150156e-01 1881:8.1223615e-02 1972:7.4915648e-02 2009:1.0867031e-01 2021:2.8201237e-01 2088:7.3738441e-02 2098:1.7699897e-01 2099:1.7802052e-01 2122:7.5097844e-02 2141:9.2161000e-02 2264:1.1130272e-01 2362:4.8006292e-02 2550:9.7868025e-02 2564:1.2834764e-01 2607:9.5717140e-02 2820:1.3676767e-01 3033:9.1752172e-02 3198:9.4323657e-02 3272:1.0240593e-01 3410:1.0618065e-01 3512:8.5455991e-02 3574:9.5631681e-02 3675:1.1940711e-01 4678:1.2296332e-01 5206:1.1661388e-01 6147:1.8070130e-01 7541:1.3165449e-01 7840:2.9703492e-01 8440:1.2342619e-01 10988:1.2549973e-01 11049:1.4151667e-01 11168:1.6530864e-01 31847:2.9991940e-01\r\n",
      "-1 |f 5:5.3301733e-02 9:6.2555432e-02 11:7.3172815e-02 13:4.9635198e-02 15:6.5190420e-02 17:2.8164724e-02 18:3.5669319e-02 19:5.5451479e-02 20:4.1486122e-02 24:1.5185564e-02 26:3.9898835e-02 33:3.7937440e-02 36:6.0122520e-02 45:6.7186847e-02 50:4.8080895e-02 51:5.1502939e-02 52:7.0759192e-02 54:2.6368383e-02 56:5.5773970e-02 61:1.0710372e-01 67:5.0984230e-02 69:3.4223195e-02 74:3.9380819e-02 80:5.8718670e-02 82:1.1237274e-01 85:4.5723863e-02 90:6.8084776e-02 102:6.4724497e-02 107:4.8656568e-02 108:6.8869792e-02 111:5.3116240e-02 112:5.0638448e-02 117:4.5061130e-02 125:8.0561966e-02 144:5.3790558e-02 191:4.7557503e-02 210:4.0814247e-02 215:5.1698022e-02 222:6.0015813e-02 224:3.7807085e-02 231:4.1280117e-02 233:3.0302791e-02 234:6.7837529e-02 255:1.0167062e-01 259:4.0633537e-02 284:5.4960225e-02 286:7.0582502e-02 301:7.8823715e-02 318:5.8683388e-02 327:6.8761639e-02 348:7.2057448e-02 365:8.7477766e-02 370:5.2241467e-02 376:7.7616692e-02 381:4.9030535e-02 400:5.0423048e-02 417:7.5442202e-02 433:3.4443535e-02 451:4.4488866e-02 464:7.3166363e-02 465:6.4148359e-02 466:7.3074237e-02 478:1.2345938e-01 481:4.7366023e-02 494:4.9308177e-02 511:1.0614729e-01 519:8.6390451e-02 526:8.3817840e-02 532:6.1101902e-02 533:2.9652225e-02 546:1.1871039e-01 549:1.4348567e-01 559:1.0908782e-01 560:5.8591850e-02 573:3.9176021e-02 574:6.7587882e-02 586:7.1268670e-02 608:9.2703134e-02 613:7.3961660e-02 620:9.6122473e-02 627:9.5267519e-02 629:5.5523060e-02 642:4.6099242e-02 668:7.0150040e-02 679:5.1829111e-02 688:8.4508322e-02 702:4.0725272e-02 720:1.3691705e-01 751:6.5293394e-02 760:6.7710258e-02 773:7.5253464e-02 785:1.2002083e-01 842:2.9899737e-02 858:7.6270103e-02 911:5.5175625e-02 995:8.0719247e-02 1011:5.1447678e-02 1014:4.8865855e-02 1071:9.0600483e-02 1127:9.2416152e-02 1145:4.6786323e-02 1194:7.1459286e-02 1326:8.7687232e-02 1399:1.2394845e-01 1452:5.1346365e-02 1554:5.5196017e-02 1654:6.1491653e-02 1710:1.0370397e-01 1725:1.3605677e-01 1746:2.9548144e-01 1756:6.8078578e-02 1810:6.7248575e-02 1930:9.8724589e-02 1935:6.2996961e-02 1964:7.8700081e-02 2014:7.2563030e-02 2125:8.7343983e-02 2201:8.5712083e-02 2293:8.2440317e-02 2298:2.1134043e-02 2322:7.2249226e-02 2338:6.4280219e-02 2514:9.5954128e-02 2728:9.1501616e-02 2795:8.0325022e-02 2885:1.5883781e-01 2893:7.9524823e-02 2923:8.8810444e-02 3158:8.9602776e-02 3180:1.0103431e-01 3184:1.3113359e-01 3437:1.4367227e-01 4731:1.0641657e-01 5168:1.0036340e-01 5197:1.0327362e-01 5424:7.7506967e-02 5972:1.4931364e-01 5984:1.0603356e-01 7993:1.1169191e-01 10050:1.3650699e-01 18686:1.4489010e-01 19584:1.4415240e-01 20267:1.3498561e-01 40078:1.4924034e-01\r\n",
      "-1 |f 13:2.9609846e-02 18:3.6027648e-02 32:4.4532869e-02 45:2.8438151e-02 64:2.8068403e-02 107:4.9145367e-02 140:2.4154101e-02 238:8.3709799e-02 276:3.6741246e-02 307:6.9328450e-02 357:5.9683714e-02 365:3.7026681e-02 413:1.0246231e-01 423:1.0169387e-01 476:9.6615113e-02 506:4.0256724e-02 524:5.8727089e-02 587:5.0132606e-02 626:6.6554219e-02 660:3.1878307e-02 664:5.3924501e-02 676:6.8776548e-02 726:4.8896302e-02 812:7.0979826e-02 820:8.4536858e-02 1024:6.3699096e-02 1029:4.8744757e-02 1062:6.1086554e-02 1218:7.4180119e-02 1426:4.6529077e-02 1429:6.4088009e-02 1486:6.5359414e-02 1489:1.5872289e-01 1609:9.3300425e-02 1731:5.4757904e-02 1772:8.5183427e-02 1885:1.0124429e-01 2131:7.7084497e-02 2202:8.4645070e-02 2221:1.1719659e-01 2223:6.4569637e-02 2226:1.4628333e-01 2311:6.7074485e-02 2423:1.2627970e-01 2580:1.4240469e-01 2731:7.3689789e-02 2871:7.5391024e-02 2893:8.0323726e-02 2940:7.7242814e-02 3617:1.3908839e-01 3756:9.6985392e-02 3772:6.5106317e-02 3841:5.4847423e-02 3855:1.6438945e-01 3864:2.0057572e-01 4132:8.9075580e-02 4276:6.6121101e-02 4340:9.7786203e-02 4595:1.0477127e-01 5261:2.0169328e-01 5295:9.2872404e-02 5300:1.0756692e-01 5775:8.7651499e-02 7559:1.5180370e-01 9840:1.2444526e-01 10622:1.0936097e-01 11306:1.2710032e-01 11408:1.4327782e-01 13492:1.3772996e-01 13494:3.6511117e-01 15759:1.1569903e-01 16056:1.7845926e-01 17679:1.6165386e-01 22275:2.2374372e-01 33393:1.5838674e-01 37562:1.8201114e-01 44781:2.4165982e-01 44966:1.6823532e-01\r\n",
      "1 |f 39:6.0213052e-02 70:4.6044845e-02 101:8.8076606e-02 140:1.1929959e-01 169:7.2478436e-02 188:3.7470724e-02 238:1.1948375e-01 247:1.7659175e-01 259:7.2609894e-02 277:1.0287616e-01 357:1.0559076e-01 358:1.2198421e-01 387:1.0968716e-01 454:7.6134235e-02 573:1.6705349e-01 644:7.2947726e-02 645:1.1702140e-01 726:8.6505972e-02 801:1.4080049e-01 802:4.9566187e-02 968:1.3092631e-01 971:2.5020817e-01 1127:5.6058250e-02 1193:7.3045760e-02 1338:1.2727100e-01 2009:2.9745954e-01 2084:1.4342444e-01 2794:1.2485216e-01 2977:1.6319990e-01 3002:1.5451294e-01 6320:2.3330823e-01 13609:6.7062819e-01\r\n"
     ]
    }
   ],
   "source": [
    "! head logistic.train.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating quadratic features for pairs: ff \n",
      "using l2 regularization = 0.1\n",
      "final_regressor = result2.vw.bin\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = logistic.train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.719974 0.719974            1            1.0   1.0000  -0.0530     1275\n",
      "0.908148 1.096321            2            2.0  -1.0000   0.6897     5356\n",
      "0.746713 0.585279            4            4.0  -1.0000   0.2299     9045\n",
      "0.667753 0.588792            8            8.0  -1.0000  -1.9259    10585\n",
      "0.558648 0.449542           16           16.0   1.0000  -0.0777      276\n",
      "0.654712 0.750777           32           32.0  -1.0000  -0.2246      496\n",
      "0.681594 0.708476           64           64.0  -1.0000  -0.5785     1830\n",
      "0.654111 0.626627          128          128.0   1.0000  -0.5068     5565\n",
      "0.647276 0.640442          256          256.0   1.0000  -0.0875     7503\n",
      "0.637992 0.628708          512          512.0  -1.0000  -0.3026     2775\n",
      "0.646040 0.654088         1024         1024.0  -1.0000  -0.3932     4950\n",
      "0.653057 0.660074         2048         2048.0   1.0000  -0.0789      496\n",
      "0.664813 0.676570         4096         4096.0  -1.0000  -0.1156     6555\n",
      "0.670544 0.676275         8192         8192.0  -1.0000  -0.0485     5356\n",
      "0.676099 0.681654        16384        16384.0   1.0000   0.0386     1225\n",
      "0.681075 0.686050        32768        32768.0   1.0000  -0.0582     5356\n",
      "0.684050 0.687025        65536        65536.0   1.0000   0.0746      561\n",
      "0.686452 0.688854       131072       131072.0   1.0000   0.0057     1275\n",
      "0.688238 0.690025       262144       262144.0  -1.0000  -0.0064     1225\n",
      "0.689515 0.690791       524288       524288.0  -1.0000  -0.0088     5671\n",
      "\n",
      "finished run\n",
      "number of examples = 781265\n",
      "weighted example sum = 781265.000000\n",
      "weighted label sum = -40183.000000\n",
      "average loss = 0.690108\n",
      "best constant = -0.102957\n",
      "best constant's loss = 0.691824\n",
      "total feature number = 3482766052\n",
      "62.20user 2.42system 0:54.39elapsed 118%CPU (0avgtext+0avgdata 21992maxresident)k\n",
      "0inputs+4104outputs (0major+2282minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "! time vw --normal_weights --quadratic ff --loss_function logistic --l2 0.1 logistic.train.vw --final_regressor result2.vw.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: даже с квадратичным размером оригинальный фичей, кролик не загнулся, а справился благодаря хешированию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating quadratic features for pairs: ff \n",
      "only testing\n",
      "predictions = preds2.txt\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = logistic.test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.989140 0.989140            1            1.0  -1.0000  -0.0054      820\n",
      "0.999102 1.009063            2            2.0   1.0000  -0.0045     2775\n",
      "0.993647 0.988192            4            4.0  -1.0000  -0.0053      946\n",
      "0.987501 0.981356            8            8.0  -1.0000  -0.0063      861\n",
      "0.989760 0.992019           16           16.0   1.0000  -0.0014      561\n",
      "0.988507 0.987254           32           32.0  -1.0000  -0.0308    21945\n",
      "0.995975 1.003442           64           64.0   1.0000  -0.0033      300\n",
      "0.996335 0.996696          128          128.0   1.0000  -0.0027      990\n",
      "0.995209 0.994083          256          256.0  -1.0000  -0.0121     1176\n",
      "0.993871 0.992534          512          512.0   1.0000  -0.0018      528\n",
      "0.995356 0.996841         1024         1024.0  -1.0000  -0.0125     7626\n",
      "0.992899 0.990442         2048         2048.0   1.0000  -0.0047     2016\n",
      "0.993372 0.993845         4096         4096.0  -1.0000  -0.0096     4851\n",
      "0.993850 0.994329         8192         8192.0  -1.0000  -0.0046     2016\n",
      "0.993311 0.992771        16384        16384.0  -1.0000  -0.0225    25651\n",
      "\n",
      "finished run\n",
      "number of examples = 23149\n",
      "weighted example sum = 23149.000000\n",
      "weighted label sum = -1577.000000\n",
      "average loss = 0.993462\n",
      "best constant = -0.068124\n",
      "best constant's loss = 0.995359\n",
      "total feature number = 103945802\n"
     ]
    }
   ],
   "source": [
    "! vw logistic.test.vw -t -i result2.vw.bin --loss_function=logistic --link=logistic -p preds2.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.005445\r\n",
      "-0.004521\r\n",
      "-0.006548\r\n",
      "-0.005296\r\n",
      "-0.006130\r\n",
      "-0.016499\r\n",
      "-0.008593\r\n",
      "-0.006277\r\n",
      "-0.007001\r\n",
      "-0.012910\r\n"
     ]
    }
   ],
   "source": [
    "! head preds2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сбор датасета для обучения VW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем составлять датасет, основываясь на данных в предоставленных таблицах от Авито (смотри начало ноутбука). \n",
    "Таблицы с исходными данными достаточно большие, поэтому будем использовать Spark для предобработки и подготовки датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Напоминалка__: Необходимо подключиться к головной машине через ssh, открыть файл `/usr/bin/anaconda/lib/python2.7/site-packages/nbformat/_version.py` и заменить 5 на 4. После этого остается перезагрузить Jupyter через ambari."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распарсим таблицы в HDFS и зарегистрируем таблицы, чтобы можно было использовать Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locationDf = spark.read.option(\"delimiter\", \"\\t\").csv('/avito/Location.tsv', header=True, inferSchema=True)\n",
    "adsInfoDf = spark.read.option(\"delimiter\", \"\\t\").csv('/avito/AdsInfo.tsv', header=True, inferSchema=True)\n",
    "categoryDf = spark.read.option(\"delimiter\", \"\\t\").csv('/avito/Category.tsv', header=True, inferSchema=True)\n",
    "phoneRequestStreamDf = spark.read.option(\"delimiter\", \"\\t\").csv('/avito/PhoneRequestsStream.tsv', header=True, inferSchema=True)\n",
    "searchInfoDf = spark.read.option(\"delimiter\", \"\\t\").csv('/avito/SearchInfo.tsv', header=True, inferSchema=True)\n",
    "visitStreamDf = spark.read.option(\"delimiter\", \"\\t\").csv('/avito/VisitsStream.tsv', header=True, inferSchema=True)\n",
    "userInfoDf = spark.read.option(\"delimiter\", \"\\t\").csv('/avito/UserInfo.tsv', header=True, inferSchema=True)\n",
    "searchStreamDf = spark.read.option(\"delimiter\", \"\\t\").csv('/avito/trainSearchStream.tsv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locationDf.registerTempTable('location')\n",
    "adsInfoDf.registerTempTable('ads_info')\n",
    "categoryDf.registerTempTable('category')\n",
    "phoneRequestStreamDf.registerTempTable('phone_request_stream')\n",
    "searchInfoDf.registerTempTable('search_info')\n",
    "visitStreamDf.registerTempTable('visit_stream')\n",
    "userInfoDf.registerTempTable('user_info')\n",
    "searchStreamDf.registerTempTable('search_stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = spark.sql(\"\"\"\n",
    "select count(*) from search_stream\n",
    "\"\"\")\n",
    "r.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично! Теперь мы осталось взглянуть на данные и попробовать собрать что-то из таблиц.\n",
    "\n",
    "Для самого простого примера возьмем все контекстуальные показы рекламы и возьмем для них два вещественных признака - HistCTR и Price. Так как информация по цене находится в другой таблице, нам потребуется приджойнить ее к основной таблице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result  = spark.sql(\"\"\"\n",
    "select s.IsClick, s.HistCTR, ai.price\n",
    "from search_stream s\n",
    "join ads_info ai on s.AdID = ai.AdID\n",
    "where ai.IsContext = True\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось привести это в нужный формат и можно сохранять"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vw(row):\n",
    "    return \"{label} |f hist:{hist} price:{price}\".format(\n",
    "        label=1 if row.IsClick == 1 else -1, \n",
    "        hist=row.HistCTR, \n",
    "        price=row.price or 0\n",
    "    )\n",
    "\n",
    "result.rdd.map(make_vw).saveAsTextFile(\"wasb:///avito/dataset-v1.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После завершения задачи, перебираемся на головную машину и выкачиваем получившийся датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -getmerge /avito/dataset-v1.data dataset-v1.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительно разделим на train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat dataset-v1.data | wc -l\n",
    "! head -n 100000 dataset-v1.data > test-v1.data\n",
    "! tail -n +100000 dataset-v1.data > train-v1.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно запускать VW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! vw train-v1.data --loss_function=logistic --link=logistic --final_regressor result-v1.vw.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! vw --testonly --initial_regressor result-v1.vw.bin --loss_function=logistic --link=logistic --predictions predictions-v1.txt test-v1.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте накинем туда еще каких-нибудь категориальных и BOW от поискового текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "resul = spark.sql(\"\"\"\n",
    "select s.IsClick, s.HistCTR, ai.Price, si.SearchQuery, si.IsUserLoggedOn, ui.UserDeviceID, ui.UserAgentOSID\n",
    "from search_stream s\n",
    "join ads_info ai on s.AdID = ai.AdID\n",
    "join search_info si on si.SearchID = s.SearchID\n",
    "join user_info ui on ui.UserID = si.UserID\n",
    "where ai.IsContext = True\n",
    "\"\"\")\n",
    "\n",
    "def make_vw(row):\n",
    "    return \"{label} |f hist:{hist} price:{price} {categr}\".format(\n",
    "        label=1 if row.IsClick == 1 else -1, \n",
    "        hist=\"{0:.5f}\".format(row.HistCTR), \n",
    "        price=row.Price or 0,\n",
    "        categr=\" \".join([\n",
    "                \"UserLoggedOn\" if row.IsUserLoggedOn else \"UserLoggedOff\",\n",
    "                \"u{}\".format(row.UserDeviceID), \n",
    "                *re.findall(r'[a-zA-Z]+', (row.SearchQuery or \"\"))\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "resul.rdd.map(make_vw).saveAsTextFile(\"wasb:///avito/dataset-v2.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогоним VW на этом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -getmerge /avito/dataset-v2.data dataset-v2.data\n",
    "! head -n 100000 dataset-v2.data > test-v2.data\n",
    "! tail -n +100000 dataset-v2.data > train-v2.data\n",
    "! vw train-v2.data --loss_function=logistic --link=logistic --final_regressor result-v2.vw.bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! vw --testonly --initial_regressor result-v2.vw.bin --loss_function=logistic --link=logistic --predictions predictions-v2.txt test-v2.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задачи**\n",
    "\n",
    "Note: не забудьте, что существует такая классная штука, как Spark \n",
    "\n",
    "* Посчитать самые популярные слова в запросах (SearchQuery)\n",
    "* Подсчитать количество объявлений для каждой категории второго уровня (level = 2) (категории имеют иерархическую структуру - все объявления в подкатегориях (3 уровень) также должны считаться объявлением в соответствующей категории второго уровня)\n",
    "* Для каждого слова из заголовка объявления подсчитать его среднюю стоимость (если слово появилось в заголовке рекламы с ценой A и в заголовке рекламы с ценой B, то его средняя стоимость - (A+B)/2)\n",
    "* Найти самый популярный фильтр (ключ в словаре SearchParams)\n",
    "* Топ 5 слов в пользовательских запросах, в которых пользователь кликнул на рекламу\n",
    "\n",
    "* Собрать датасет, состоящий из не менее 8 различных фичей (логически различных - в последнем примере фичей было 5 - histCRT, price, user_login, user_device, search_query_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
